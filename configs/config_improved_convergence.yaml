# ABR Hierarchical U-Net Configuration - IMPROVED CONVERGENCE
# Addresses training dynamics issues: proper convergence over multiple epochs

project:
  name: "abr_hierarchical_unet"
  experiment_name: "improved_convergence"
  description: "Fixed training dynamics for proper multi-epoch convergence"

# Paths
paths:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  output_dir: "outputs"

# Data Configuration
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  
  # Patient-stratified splitting
  splitting:
    split_type: "patient_stratified"
    train_ratio: 0.75
    val_ratio: 0.15
    test_ratio: 0.10
    random_seed: 42
  
  # OPTIMIZED Data loading for better convergence
  dataloader:
    batch_size: 16         # REDUCED: Smaller batches for more precise gradients
    num_workers: 4
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 2
    persistent_workers: true

# Model Architecture - Optimized for gradual learning
model:
  type: "hierarchical_unet"
  
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 64      # Moderate size for controlled learning
    n_levels: 3           
    n_classes: 5
    dropout: 0.1          
    
    # S4 Encoder
    encoder:
      n_s4_layers: 2      
      d_state: 64         
      use_enhanced_s4: true
      
    # Transformer Decoder
    decoder:
      n_transformer_layers: 2
      n_heads: 8
      use_multi_scale_attention: true
      use_cross_attention: true
      
    # FiLM conditioning
    film:
      hidden_dim: 64
      dropout: 0.1
      
    # Output heads
    outputs:
      signal_activation: "tanh"
      peak_head_dim: 128
      class_head_dim: 128
      threshold_head_dim: 128
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration
diffusion:
  noise_schedule:
    type: "cosine"
    num_timesteps: 1000
    peak_preserve_ratio: 0.3
    beta_start: 1e-4
    beta_end: 0.02
  
  sampling:
    type: "ddpm"
    ddim_eta: 0.0
    num_sampling_steps: 50
    temperature: 1.0
    clip_denoised: true

# IMPROVED Training Configuration - PROPER CONVERGENCE
training:
  # PHASE-BASED Optimizer Configuration
  optimizer:
    type: "adamw"
    learning_rate: 1e-5     # MUCH LOWER: Start gentle for controlled learning
    weight_decay: 1e-6      # Reduced weight decay
    betas: [0.9, 0.999]     # Standard betas for stability
    eps: 1e-8
    amsgrad: false
  
  # SOPHISTICATED Learning Rate Scheduler
  scheduler:
    type: "cosine_annealing_warm_restarts"
    T_0: 20                 # Shorter cycles for frequent adaptation
    T_mult: 1               # No cycle lengthening initially
    eta_min: 1e-8          # Very low minimum LR
    warmup_epochs: 5        # Gentle warmup
    warmup_type: "linear"   # Linear warmup
  
  # Training Parameters - OPTIMIZED FOR CONVERGENCE
  epochs: 100              # More epochs for gradual learning
  gradient_clip: 0.5       # Moderate clipping
  accumulation_steps: 8    # HIGH accumulation for stable gradients
  log_frequency: 10        # More frequent monitoring
  
  # Validation and checkpointing
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 5
    save_best: true
    save_last: true
    save_dir: "checkpoints"
  
  # Early stopping - LESS AGGRESSIVE
  early_stopping:
    patience: 20            # Allow more time for gradual improvement
    min_delta: 1e-6         # Smaller threshold for improvement
    monitor: "val_total_loss"
    mode: "min"

# PROGRESSIVE Loss Configuration - KEY FOR PROPER CONVERGENCE
loss:
  type: "abr_diffusion"
  
  # CAREFULLY BALANCED weights - PROGRESSIVE APPROACH
  weights:
    diffusion: 1.0
    peak_exist: 0.3        # Moderate importance
    peak_latency: 0.2      # Controlled weight
    peak_amplitude: 0.01   # Keep very low (was problematic)
    classification: 0.8    # Important but not dominant
    threshold: 0.1         # HEAVILY REDUCED (was causing plateau)
  
  # Loss settings
  peak_loss_type: "smooth_l1"  # Smooth loss for better gradients
  huber_delta: 0.5
  
  # Class balancing - MODERATE
  class_weights: "balanced"
  focal_loss:
    use_focal: true        # Enable for better class balance
    alpha: 0.5            # Moderate alpha
    gamma: 1.5            # Moderate gamma (not too aggressive)
  
  # Perceptual loss - DISABLED initially
  perceptual:
    use_perceptual: false

# Evaluation Configuration
evaluation:
  metrics:
    signal_metrics: ["mse", "mae", "correlation"]
    peak_metrics: ["peak_mae", "peak_accuracy"]
    classification_metrics: ["accuracy", "f1_macro"]
    threshold_metrics: ["threshold_mae"]
  
  visualization:
    save_plots: true
    plot_types: ["reconstruction", "peaks"]
    n_samples_plot: 5

# Inference Configuration
inference:
  num_samples: 100
  batch_size: 8           # Smaller batch for inference
  temperature: 1.0
  guidance_scale: 1.0
  use_cfg: true
  
  conditioning:
    age_range: [0, 100]
    gender_range: [0, 1]
    ear_range: [0, 1]
    stimulus_range: [0, 100]
    
  output_dir: "outputs/inference"
  save_statistics: true
  save_formats: ["npy", "csv"]

# Hardware Configuration
hardware:
  device: "auto"
  mixed_precision: true
  compile_model: false
  channels_last: false
  multi_gpu: false

# Logging Configuration
logging:
  level: "INFO"
  use_wandb: true         # Enable for better monitoring
  wandb:
    project: "abr_improved_convergence"
    entity: null
    tags: ["improved_convergence", "proper_dynamics"]

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false

# ADVANCED Optimization - CONVERGENCE FOCUSED
optimization:
  # Data augmentation - LIGHT for stability
  augmentation:
    use_augmentation: true
    noise_factor: 0.005     # Very light noise
    time_stretch_factor: 0.05  # Minimal stretching
    amplitude_scale_factor: 0.05  # Minimal scaling
  
  # Model optimizations
  model_optimizations:
    use_gradient_checkpointing: true  # Save memory for smaller batches
    use_efficient_attention: false
    use_flash_attention: false
  
  # Training optimizations
  training_optimizations:
    use_compile: false
    use_channels_last: false
    use_fused_optimizer: false

# ADAPTIVE Learning Strategy
adaptive_learning:
  # Monitor convergence and adjust
  monitor_metrics: ["val_total_loss", "val_signal_loss", "val_class_loss"]
  
  # Learning rate adaptation
  lr_adaptation:
    enable: true
    patience: 8             # Reduce LR if no improvement for 8 epochs
    factor: 0.5             # Halve the learning rate
    min_lr: 1e-8           # Minimum learning rate
    
  # Loss weight adaptation
  weight_adaptation:
    enable: true
    frequency: 10          # Check every 10 epochs
    threshold_max_ratio: 10.0  # Max ratio of threshold to other losses
    
  # Batch size adaptation
  batch_adaptation:
    enable: false          # Disable for now (experimental)
    start_size: 16
    min_size: 8
    max_size: 32
# Enhanced ABR Diffusion Training Configuration for Google Colab
# Optimized for V100/T4 GPUs with all improvements

data:
  path: /content/drive/MyDrive/abr-project/data/processed/ultimate_dataset_with_clinical_thresholds.pkl
  batch_size: 64                # Reduced for Colab memory constraints
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  num_workers: 2               # Lower for Colab
  pin_memory: true
  persistent_workers: false    # Disabled for Colab stability
  prefetch_factor: 2

model:
  # Core architecture
  signal_length: 200
  static_dim: 4
  input_channels: 1
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  
  # S4 configuration
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Transformer configuration
  num_transformer_layers: 2
  num_heads: 8
  use_multi_scale_attention: true
  use_cross_attention: true
  
  # Regularization
  dropout: 0.1
  film_dropout: 0.15
  
  # Enhanced architecture features
  use_enhanced_model: true     # Use ImprovedHierarchicalUNet
  timestep_dim: 128           # Timestep embedding dimension
  context_dim: 256            # Global context dimension
  use_dual_branch: true       # Envelope + detail branches
  use_global_context: true    # Global context conditioning
  use_v_prediction: true      # V-prediction instead of noise
  
  # Architecture flags (CRITICAL - must match training)
  use_cfg: true
  use_attention_heads: true
  predict_uncertainty: false    # Disable for simplicity
  enable_joint_generation: false # Focus on signal generation only
  use_task_specific_extractors: true
  use_attention_skip_connections: true
  channel_multiplier: 2.0
  
  # Output configuration
  use_robust_heads: true

diffusion:
  schedule_type: cosine        # Cosine schedule for better sampling
  num_timesteps: 1000
  clip_denoised: false        # Disable clipping for better dynamics
  eta: 0.0                    # Deterministic DDIM sampling
  prediction_type: v_prediction # Use v-prediction for better convergence
  use_p2_weighting: true      # P2 loss weighting for stability
  p2_k: 1.0                   # P2 weighting parameter k
  p2_gamma: 1.0               # P2 weighting parameter gamma

training:
  epochs: 100                 # Reduced for Colab time limits
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_epochs: 5            # Reduced warmup
  early_stopping_patience: 20 # Reduced patience
  accumulation_steps: 2       # Gradient accumulation for larger effective batch
  mixed_precision: true
  grad_clip_norm: 1.0

optimization:
  ema_decay: 0.999
  scheduler_type: cosine_annealing

evaluation:
  ddim_steps: 50              # Reduced for faster evaluation
  guidance_scale: 1.0
  num_preview_samples: 4      # Reduced for faster logging

logging:
  checkpoint_dir: /content/drive/MyDrive/abr-project/checkpoints/enhanced_colab
  log_dir: /content/drive/MyDrive/abr-project/logs/enhanced_colab
  use_tensorboard: true
  use_wandb: false            # Disabled for simplicity
  log_interval: 100
  eval_interval: 5            # Evaluate every 5 epochs
  checkpoint_interval: 10     # Save every 10 epochs

# Colab-specific settings
colab:
  save_to_drive: true
  mount_path: /content/drive
  project_path: /content/drive/MyDrive/abr-project
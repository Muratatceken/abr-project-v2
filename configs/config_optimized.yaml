# ABR Hierarchical U-Net Optimized Configuration
# Optimized hyperparameters for best training results and performance

project:
  name: "ABR_HierarchicalUNet_Optimized"
  description: "Optimized Hierarchical U-Net with S4 Encoder and Transformer Decoder for ABR Signal Generation"
  version: "2.0.0"
  author: "AI Assistant"

# Data Configuration - Optimized for better performance
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4  # age, intensity, stimulus_rate, fmp
  n_classes: 5   # hearing loss types
  
  # Optimized data splits for better generalization
  splits:
    train_ratio: 0.75  # Increased training data
    val_ratio: 0.15
    test_ratio: 0.10   # Slightly reduced test set
    random_seed: 42
  
  # Optimized data loading for faster training
  dataloader:
    batch_size: 48      # Increased for better gradient estimates
    num_workers: 6      # More workers for faster data loading
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 3  # Added for better performance
    persistent_workers: true  # Added for efficiency

# Model Architecture - Enhanced for better performance
model:
  type: "hierarchical_unet"
  
  # Optimized architecture parameters
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 80      # Increased from 64 for more capacity
    n_levels: 4
    n_classes: 5
    dropout: 0.15          # Increased for better regularization
    
    # Enhanced S4 Encoder settings
    encoder:
      n_s4_layers: 3       # Increased from 2 for more depth
      d_state: 80          # Increased from 64 for more capacity
      use_enhanced_s4: true
      
    # Enhanced Transformer Decoder settings
    decoder:
      n_transformer_layers: 3  # Increased from 2 for more depth
      n_heads: 8              # Fixed: d_model (160) must be divisible by n_heads
      use_multi_scale_attention: true
      use_cross_attention: true
      
    # Enhanced FiLM conditioning
    film:
      hidden_dim: 96       # Increased from 64
      dropout: 0.15        # Added FiLM dropout
      
    # Enhanced output heads
    outputs:
      signal_activation: "tanh"
      peak_head_dim: 256   # Increased from 128
      class_head_dim: 256  # Increased from 128
      threshold_head_dim: 256  # Increased from 128
      use_attention_heads: true
      predict_uncertainty: true

# Optimized Diffusion Configuration
diffusion:
  # Optimized noise schedule
  noise_schedule:
    type: "cosine"         # Best for most tasks
    num_timesteps: 1000
    peak_preserve_ratio: 0.4  # Increased from 0.3
    beta_start: 1e-4       # Adjusted for better stability
    beta_end: 0.02         # Adjusted for better range
  
  # Enhanced sampling
  sampling:
    type: "ddpm"           # Keep DDPM for training stability
    ddim_eta: 0.0
    num_sampling_steps: 100  # Increased for better quality
    temperature: 0.9       # Slight temperature scaling
    clip_denoised: true
    
    # Enhanced clinical constraints
    constraints:
      apply_constraints: true
      peak_latency_range: [0.8, 8.5]  # Slightly expanded
      amplitude_range: [-0.6, 0.6]    # Slightly expanded
      smooth_kernel_size: 3            # Reduced for less smoothing

# Optimized Training Configuration
training:
  # Enhanced optimizer settings
  optimizer:
    type: "adamw"
    learning_rate: 2e-4    # Increased from 1e-4 for faster convergence
    weight_decay: 1e-4     # Increased for better regularization
    betas: [0.9, 0.95]     # Optimized betas
    eps: 1e-8
    amsgrad: false         # Disabled for faster training
  
  # Optimized learning rate scheduler
  scheduler:
    type: "cosine_annealing_warm_restarts"
    T_0: 30               # Reduced for more frequent restarts
    T_mult: 2             # Fixed: Must be integer >= 1
    eta_min: 5e-7         # Lower minimum LR
    warmup_epochs: 5      # Reduced warmup
    warmup_type: "linear" # Added warmup type
  
  # Enhanced training parameters
  epochs: 300             # Increased for better convergence
  gradient_clip: 0.5      # Reduced for better stability
  accumulation_steps: 2   # Added gradient accumulation
  log_frequency: 25       # More frequent logging
  
  # Enhanced validation and checkpointing
  validation:
    frequency: 1
    compute_metrics: true
    save_predictions: true  # Added prediction saving
  
  checkpointing:
    save_frequency: 5     # More frequent saves
    save_best: true
    save_last: true
    save_top_k: 3         # Save top 3 models
    save_dir: "checkpoints"
    monitor_metric: "val_total_loss"
  
  # Enhanced early stopping
  early_stopping:
    patience: 50          # Increased patience
    min_delta: 1e-5       # Increased threshold
    monitor: "val_total_loss"
    mode: "min"
    restore_best_weights: true

# Optimized Loss Configuration
loss:
  type: "abr_diffusion"
  
  # Optimized loss weights for better balance
  weights:
    diffusion: 1.0
    peak_exist: 0.8       # Increased importance
    peak_latency: 1.2     # Increased importance
    peak_amplitude: 1.2   # Increased importance
    classification: 1.5   # Increased importance
    threshold: 1.0        # Increased importance
  
  # Enhanced loss settings
  peak_loss_type: "smooth_l1"  # Better than huber for peaks
  huber_delta: 0.5             # Reduced delta
  
  # Enhanced class imbalance handling
  class_weights: "balanced"
  focal_loss:
    use_focal: true      # Enabled for better class balance
    alpha: 0.75          # Optimized alpha
    gamma: 2.5           # Increased gamma
  
  # Enhanced perceptual loss
  perceptual:
    use_perceptual: true  # Enabled for better signal quality
    feature_weights:
      peak_preservation: 3.0  # Increased importance
      morphology: 1.5         # Increased importance
      spectral: 1.0           # Increased importance
      temporal: 2.0           # Increased importance

# Enhanced Evaluation Configuration
evaluation:
  metrics:
    # Enhanced signal quality metrics
    signal_metrics: ["mse", "mae", "correlation", "dtw_distance", "snr", "spectral_distance"]
    
    # Enhanced peak prediction metrics
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1", "peak_correlation", "timing_accuracy"]
    
    # Enhanced classification metrics
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted", "confusion_matrix", "precision", "recall"]
    
    # Enhanced threshold metrics
    threshold_metrics: ["threshold_mae", "threshold_r2", "threshold_correlation", "clinical_accuracy"]
  
  # Enhanced visualization
  visualization:
    save_plots: true
    plot_types: ["reconstruction", "peaks", "latent_space", "generation_samples", "loss_curves", "attention_maps"]
    n_samples_plot: 20     # More samples for better visualization
    save_animations: true  # Added animation saving
    
  # Enhanced output
  output_dir: "outputs/evaluation"
  save_detailed_results: true
  save_predictions: true
  save_embeddings: true

# Enhanced Inference Configuration
inference:
  # Enhanced generation parameters
  generation:
    num_samples: 200       # Increased for better statistics
    batch_size: 24         # Increased batch size
    temperature: 0.9       # Optimized temperature
    guidance_scale: 1.2    # Added guidance scaling
    use_cfg: true          # Classifier-free guidance
    
  # Enhanced conditional generation
  conditioning:
    age_range: [18, 85]    # Expanded range
    intensity_range: [55, 105]  # Expanded range
    rate_range: [8, 85]    # Expanded range
    fmp_range: [0.4, 1.0]  # Expanded range
    
  # Enhanced output
  output_dir: "outputs/inference"
  save_formats: ["signals", "peaks", "predictions", "metadata", "embeddings"]
  save_statistics: true

# Enhanced Logging and Monitoring
logging:
  level: "INFO"
  log_dir: "logs"
  use_tensorboard: true
  use_wandb: true          # Enabled for better tracking
  
  # Enhanced Weights & Biases configuration
  wandb:
    project: "abr-hierarchical-unet-optimized"
    entity: null
    tags: ["diffusion", "abr", "s4", "transformer", "optimized", "multi-task"]
    group: "production"
    notes: "Optimized configuration for best performance"

# Enhanced Reproducibility
reproducibility:
  seed: 42
  deterministic: true      # Enabled for reproducible results
  benchmark: false         # Disabled for deterministic behavior

# Enhanced Hardware Configuration
hardware:
  device: "auto"           # auto, cuda, cpu
  gpu_id: 0
  mixed_precision: true
  compile_model: true      # Enabled PyTorch 2.0 compilation
  channels_last: true      # Memory optimization
  
# Enhanced Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints"
  output_dir: "outputs"
  log_dir: "logs"

# Advanced Optimization Settings
optimization:
  # Data augmentation
  augmentation:
    enabled: true
    noise_std: 0.01        # Small noise augmentation
    time_stretch: 0.05     # Time stretching
    amplitude_scale: 0.1   # Amplitude scaling
    
  # Model optimization
  model_optimization:
    use_flash_attention: false  # Set to true if available
    use_gradient_checkpointing: true  # Memory optimization
    use_weight_norm: false
    
  # Training optimization
  training_optimization:
    find_unused_parameters: false
    use_automatic_optimization: true
    precision: 16          # Mixed precision level
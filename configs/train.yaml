# ABR Transformer Training Configuration
# Professional training pipeline for v-prediction diffusion

# Basic experiment settings
seed: 42
device: "cuda"  # Will fallback to CPU if CUDA unavailable
log_dir: "runs/abr_transformer"
exp_name: "abr_vpred_base"

# Dataset configuration
data:
  # Paths to CSV files used by dataset.py (update these paths)
  train_csv: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"  # Using pkl for now, adapt as needed
  val_csv: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"    # Same file, will be split
  
  # Dataset properties (must match ABRTransformerGenerator)
  sequence_length: 200
  static_order: ["Age", "Intensity", "Stimulus Rate", "FMP"]
  
  # Data splits
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

# DataLoader configuration
loader:
  batch_size: 32
  num_workers: 4
  pin_memory: true
  drop_last: true
  prefetch_factor: 2
  persistent_workers: false

# Model architecture
model:
  # ABRTransformerGenerator parameters
  input_channels: 1
  static_dim: 4
  sequence_length: 200  # Must match data.sequence_length
  
  # Transformer architecture
  d_model: 256
  n_layers: 6
  n_heads: 8
  ff_mult: 4
  dropout: 0.1
  
  # Conditioning options
  use_timestep_cond: true
  use_static_film: true

# Diffusion configuration
diffusion:
  # Training schedule
  num_train_steps: 1000
  schedule: "cosine"  # Currently only cosine supported
  v_prediction: true
  
  # Sampling/inference
  sample_steps: 60
  ddim_eta: 0.0  # Deterministic sampling
  cfg_scale: 1.0  # Classifier-free guidance scale

# Optimization configuration
optim:
  lr: 0.0001
  betas: [0.9, 0.99]
  weight_decay: 0.00001
  grad_clip: 1.0
  
  # Mixed precision and EMA
  amp: true
  ema_decay: 0.999

# Loss configuration
loss:
  # STFT loss weight (0.0 to disable)
  stft_weight: 0.15
  
  # STFT parameters
  stft:
    n_fft: 64
    hop_length: 16
    win_length: 64
    eps: 0.00000001

# Training loop configuration
trainer:
  max_epochs: 100
  log_every_steps: 50
  validate_every_epochs: 1
  
  # Sampling and visualization
  sample_every_epochs: 2
  num_val_plots: 8
  num_sample_plots: 8
  
  # Checkpointing
  ckpt_dir: "checkpoints/abr_transformer"
  save_every_epochs: 5
  keep_last_k: 5  # Keep last 5 epoch checkpoints
  resume: ""  # Path to checkpoint to resume from (empty = start fresh)
  
  # Early stopping (optional)
  early_stop_patience: 20
  early_stop_min_delta: 0.000001

# Visualization configuration
viz:
  plot_spectrogram: true
  plot_audio: false  # Disabled for ABR signals
  audio_sample_rate: 20000  # 20kHz (if audio enabled)
  
  # Plot customization
  max_waveform_plots: 8
  max_spectrogram_plots: 4
  figsize: [12, 8]

# Advanced training options
advanced:
  # Classifier-free guidance dropout during training
  cfg_dropout_prob: 0.1
  
  # Gradient monitoring
  log_grad_norm: true
  log_param_norm: true
  grad_norm_clip: 1.0
  
  # Learning rate scheduling
  lr_scheduler: "cosine"  # Options: cosine, linear, constant
  lr_warmup_steps: 1000
  
  # Validation options
  val_sample_cfg_scales: [1.0, 1.5, 2.0]  # Different CFG scales for validation

# Logging configuration
logging:
  # TensorBoard scalars
  log_loss_components: true
  log_learning_rate: true
  log_gradient_norm: true
  log_parameter_histograms: false  # Can be expensive
  
  # Console logging
  console_log_level: "INFO"
  
  # Checkpoint metadata
  save_config_with_checkpoint: true
  save_model_summary: true

# Hardware and performance
performance:
  # Memory optimization
  gradient_checkpointing: false
  pin_memory: true
  non_blocking: true
  
  # Multi-GPU (future extension)
  distributed: false
  find_unused_parameters: false

# Reproducibility
reproducibility:
  deterministic: false  # Set to true for full reproducibility (slower)
  benchmark: true      # Use cudNN benchmarking for speed

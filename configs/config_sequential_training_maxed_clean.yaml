# MAXED OUT L4 GPU Configuration - Clean Version (No CUDA Warnings)
# System: 53GB RAM, L4 GPU 22.5GB VRAM
# Fixed: cuDNN/cuBLAS registration warnings from multiple workers

project:
  name: "ABR_Sequential_Training_MAXED_CLEAN"
  description: "Maximum L4 GPU Utilization - Ultra Fast Training - No Warnings"
  version: "1.0.0"

# Data Configuration - MAXED FOR L4 GPU (Fixed worker warnings)
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4
  n_classes: 5
  
  splits:
    train_ratio: 0.75
    val_ratio: 0.15
    test_ratio: 0.10
    random_seed: 42
  
  # MAXED OUT for L4 GPU (Fixed CUDA warnings)
  dataloader:
    batch_size: 256         # MASSIVE batch size - utilize 19GB available GPU RAM
    num_workers: 8          # Reduced from 12 to prevent CUDA factory warnings
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 6      # Reduced but still aggressive prefetching
    persistent_workers: true

# Model Architecture - Optimized for large batches
model:
  type: "hierarchical_unet"
  
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 64
    n_levels: 3
    n_classes: 5
    dropout: 0.1            # Lower dropout for large batches
    
    encoder:
      n_s4_layers: 2
      d_state: 64
      use_enhanced_s4: true
      
    decoder:
      n_transformer_layers: 2
      n_heads: 8
      use_multi_scale_attention: true
      use_cross_attention: true
      
    outputs:
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration
diffusion:
  noise_schedule:
    type: "cosine"
    num_timesteps: 400
    beta_start: 1e-4
    beta_end: 0.015
  
  sampling:
    type: "ddim"
    num_sampling_steps: 25
    temperature: 1.0
    clip_denoised: true

# Training Configuration - ULTRA FAST
training:
  optimizer:
    type: "adamw"
    learning_rate: 2e-4     # Higher LR for massive batches (batch_size 256)
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  
  scheduler:
    type: "cosine_annealing"
    T_max: 10               # Very short cycles for fast training
    eta_min: 1e-8
  
  epochs: 25              # VERY SHORT: With massive batches, convergence will be MUCH faster
  gradient_clip: 1.0
  accumulation_steps: 1   # No accumulation needed with massive batches
  log_frequency: 2        # More frequent logging
  
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 2       # Frequent saves for short training
    save_best: true
    save_last: true
    save_dir: "checkpoints/sequential_maxed_clean"
  
  early_stopping:
    patience: 4             # Very short patience
    min_delta: 1e-6
    monitor: "val_total_loss"
    mode: "min"

# ULTRA FAST SEQUENTIAL TRAINING - Massive batch convergence
sequential_training:
  enabled: true
  
  phases:
    # Phase 1: Signal only - LIGHTNING FAST with batch_size 256
    - phase: "signal_only"
      epochs: 3             # ULTRA SHORT: 256 batch size = 152 batches per epoch!
      description: "Learn basic signal reconstruction and diffusion process"
      lr_multiplier: 1.0
      early_stopping_patience: 2
      enabled_tasks: ["diffusion"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 0.0
        threshold: 0.0
    
    # Phase 2: Signal + Classification - LIGHTNING FAST
    - phase: "signal_classification"
      epochs: 5             # SHORT: Massive batches = fast convergence
      description: "Add classification while maintaining signal quality"
      lr_multiplier: 0.8
      early_stopping_patience: 3
      enabled_tasks: ["diffusion", "classification"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.5
        threshold: 0.0
    
    # Phase 3: Signal + Classification + Threshold
    - phase: "signal_classification_threshold"
      epochs: 4
      description: "Add threshold regression"
      lr_multiplier: 0.6
      early_stopping_patience: 2
      enabled_tasks: ["diffusion", "classification", "threshold"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.0
        threshold: 1.0
    
    # Phase 4: Careful peak integration
    - phase: "all_tasks_careful"
      epochs: 6
      description: "Carefully add peak detection with very low weights"
      lr_multiplier: 0.4
      early_stopping_patience: 3
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.15
        peak_latency: 0.08
        peak_amplitude: 0.02
        classification: 1.8
        threshold: 0.8
    
    # Phase 5: Final balanced training
    - phase: "balanced_final"
      epochs: 7
      description: "Final balanced training of all tasks"
      lr_multiplier: 0.3
      early_stopping_patience: 3
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.2
        peak_latency: 0.1
        peak_amplitude: 0.03
        classification: 1.5
        threshold: 0.7

# Loss Configuration
loss:
  type: "abr_diffusion"
  peak_loss_type: "smooth_l1"
  huber_delta: 0.1
  
  # Base loss weights
  weights:
    diffusion: 1.0
    peak_exist: 0.1
    peak_latency: 0.05
    peak_amplitude: 0.001
    classification: 1.5
    threshold: 0.5
  
  # Enhanced class balance
  enhanced_class_balance: true
  class_weights: "balanced"
  focal_loss:
    use_focal: true
    alpha: 2.0
    gamma: 3.0

# Enhanced Monitoring
logging:
  level: "INFO"
  log_dir: "logs/sequential_maxed_clean"
  use_tensorboard: true
  use_wandb: false
  enhanced_monitoring: true
  
  detailed_logging: true
  log_model_gradients: false
  log_loss_components: true
  log_phase_transitions: true

# Evaluation
evaluation:
  evaluation_type: "diffusion"
  metrics:
    signal_metrics: ["mse", "mae", "correlation", "snr"]
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1"]
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted"]
    threshold_metrics: ["threshold_mae", "threshold_r2"]
  
  output_dir: "outputs/sequential_maxed_clean"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: false
  benchmark: true           # Enable cuDNN benchmarking for speed

# Hardware - MAXED OUT L4 (Clean)
hardware:
  device: "cuda"
  mixed_precision: true
  compile_model: true       # PyTorch 2.0 compilation for speed
  gradient_checkpointing: false  # Disabled - we have plenty of VRAM

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints/sequential_maxed_clean"
  output_dir: "outputs/sequential_maxed_clean" 
  log_dir: "logs/sequential_maxed_clean"
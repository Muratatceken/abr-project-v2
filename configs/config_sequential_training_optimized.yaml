# Optimized Sequential Training Configuration - Speed Optimized for L4 GPU
# This configuration is optimized for Colab L4 GPU (23.8GB memory)

project:
  name: "ABR_Sequential_Training_Optimized"
  description: "Sequential Training Optimized for L4 GPU"
  version: "1.0.0"

# Data Configuration - OPTIMIZED FOR L4 GPU
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4
  n_classes: 5
  
  splits:
    train_ratio: 0.75
    val_ratio: 0.15
    test_ratio: 0.10
    random_seed: 42
  
  # OPTIMIZED for L4 GPU - 4x faster training
  dataloader:
    batch_size: 64          # INCREASED: 4x larger batches (was 12)
    num_workers: 4          # INCREASED: More parallel data loading
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 4      # INCREASED: More prefetching
    persistent_workers: true

# Model Architecture - Same as before
model:
  type: "hierarchical_unet"
  
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 64
    n_levels: 3
    n_classes: 5
    dropout: 0.15
    
    encoder:
      n_s4_layers: 2
      d_state: 64
      use_enhanced_s4: true
      
    decoder:
      n_transformer_layers: 2
      n_heads: 8
      use_multi_scale_attention: true
      use_cross_attention: true
      
    outputs:
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration
diffusion:
  noise_schedule:
    type: "cosine"
    num_timesteps: 400
    beta_start: 1e-4
    beta_end: 0.015
  
  sampling:
    type: "ddim"
    num_sampling_steps: 25
    temperature: 1.0
    clip_denoised: true

# Training Configuration - OPTIMIZED FOR SPEED
training:
  optimizer:
    type: "adamw"
    learning_rate: 1e-4     # INCREASED: Higher LR for larger batches
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  
  scheduler:
    type: "cosine_annealing"
    T_max: 30               # Adjusted for faster phases
    eta_min: 1e-8
  
  epochs: 150
  gradient_clip: 1.0       # Slightly higher for larger batches
  accumulation_steps: 2    # REDUCED: Less accumulation needed with larger batches
  log_frequency: 5         # More frequent logging for faster batches
  
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 5       # More frequent saves
    save_best: true
    save_last: true
    save_dir: "checkpoints/sequential_optimized"
  
  early_stopping:
    patience: 20            # Faster convergence expected
    min_delta: 1e-6
    monitor: "val_total_loss"
    mode: "min"

# SEQUENTIAL TRAINING - OPTIMIZED PHASES
sequential_training:
  enabled: true
  
  phases:
    # Phase 1: Signal only - FASTER
    - phase: "signal_only"
      epochs: 15            # REDUCED: Faster with larger batches
      description: "Learn basic signal reconstruction and diffusion process"
      lr_multiplier: 1.0
      early_stopping_patience: 8
      enabled_tasks: ["diffusion"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 0.0
        threshold: 0.0
    
    # Phase 2: Signal + Classification - FASTER
    - phase: "signal_classification"
      epochs: 25            # REDUCED: Faster convergence
      description: "Add classification while maintaining signal quality"
      lr_multiplier: 0.8    # Higher LR for larger batches
      early_stopping_patience: 12
      enabled_tasks: ["diffusion", "classification"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.5
        threshold: 0.0
    
    # Phase 3: Signal + Classification + Threshold
    - phase: "signal_classification_threshold"
      epochs: 20
      description: "Add threshold regression"
      lr_multiplier: 0.6
      early_stopping_patience: 10
      enabled_tasks: ["diffusion", "classification", "threshold"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.0
        threshold: 1.0
    
    # Phase 4: Careful peak integration
    - phase: "all_tasks_careful"
      epochs: 25
      description: "Carefully add peak detection with very low weights"
      lr_multiplier: 0.4
      early_stopping_patience: 15
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.15
        peak_latency: 0.08
        peak_amplitude: 0.02
        classification: 1.8
        threshold: 0.8
    
    # Phase 5: Final balanced training
    - phase: "balanced_final"
      epochs: 15
      description: "Final balanced training of all tasks"
      lr_multiplier: 0.3
      early_stopping_patience: 10
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.2
        peak_latency: 0.1
        peak_amplitude: 0.03
        classification: 1.5
        threshold: 0.7

# Loss Configuration
loss:
  type: "abr_diffusion"
  peak_loss_type: "smooth_l1"
  huber_delta: 0.1
  
  # Base loss weights
  weights:
    diffusion: 1.0
    peak_exist: 0.1
    peak_latency: 0.05
    peak_amplitude: 0.001
    classification: 1.5
    threshold: 0.5
  
  # Enhanced class balance
  enhanced_class_balance: true
  class_weights: "balanced"
  focal_loss:
    use_focal: true
    alpha: 2.0
    gamma: 3.0

# Enhanced Monitoring
logging:
  level: "INFO"
  log_dir: "logs/sequential_optimized"
  use_tensorboard: true
  use_wandb: false
  enhanced_monitoring: true
  
  detailed_logging: true
  log_model_gradients: false    # Disabled for speed
  log_loss_components: true
  log_phase_transitions: true

# Evaluation
evaluation:
  evaluation_type: "diffusion"
  metrics:
    signal_metrics: ["mse", "mae", "correlation", "snr"]
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1"]
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted"]
    threshold_metrics: ["threshold_mae", "threshold_r2"]
  
  output_dir: "outputs/sequential_optimized"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: false      # Allow for speed optimizations
  benchmark: true           # Enable cuDNN benchmarking for speed

# Hardware - OPTIMIZED for L4
hardware:
  device: "cuda"
  mixed_precision: true     # AMP for speed
  compile_model: true       # PyTorch 2.0 compilation for speed

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints/sequential_optimized"
  output_dir: "outputs/sequential_optimized" 
  log_dir: "logs/sequential_optimized"
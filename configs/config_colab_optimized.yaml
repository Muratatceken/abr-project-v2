# Optimized ABR Diffusion Training Configuration for Google Colab
# Addresses architectural mismatches and improves training stability

training:
  device: cuda                 # Force CUDA on Colab
  mixed_precision: true        # Enable AMP for speed and memory efficiency
  random_seed: 42
  early_stopping_patience: 30  # Increased patience for better convergence

logging:
  checkpoint_dir: /content/drive/MyDrive/abr_checkpoints/optimized  # Save to Google Drive
  log_dir: /content/drive/MyDrive/abr_logs/optimized              # Save logs to Drive
  use_tensorboard: true
  use_wandb: false
  wandb_project: abr-diffusion-optimized
  wandb_run_name: abr_optimized_colab
  log_interval: 25             # More frequent logging
  val_interval: 1              # Validate every epoch
  val_preview_samples: 8       # Generate preview samples during validation
  save_interval: 5             # Save checkpoint every 5 epochs

data:
  path: /content/drive/MyDrive/abr_data/ultimate_dataset_with_clinical_thresholds.pkl  # Colab path
  batch_size: 128              # Increase for A100/V100
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  num_workers: 2               # Reduced for Colab
  pin_memory: true
  persistent_workers: false    # Disable for Colab stability
  prefetch_factor: 2           # Reduced for Colab

model:
  # Core architecture
  signal_length: 200
  static_dim: 4
  input_channels: 1
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  
  # S4 configuration
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Transformer configuration
  num_transformer_layers: 2
  num_heads: 8
  use_multi_scale_attention: true
  use_cross_attention: true
  
  # Regularization
  dropout: 0.1
  film_dropout: 0.15
  
  # Architecture flags (CRITICAL - must match training)
  use_cfg: true
  use_attention_heads: true
  predict_uncertainty: false    # Disable for simplicity
  enable_joint_generation: false # Focus on signal generation only
  use_task_specific_extractors: true
  use_attention_skip_connections: true
  channel_multiplier: 2.0
  
  # Output configuration
  use_robust_heads: true

diffusion:
  schedule_type: cosine        # Cosine schedule for better sampling
  num_timesteps: 1000
  clip_denoised: true         # Enable clipping during sampling
  eta: 0.0                    # Deterministic DDIM sampling

optimization:
  epochs: 150                  # Sufficient for convergence
  learning_rate: 0.0002       # Slightly higher for Colab
  weight_decay: 0.0001
  grad_clip_norm: 1.0         # Gradient clipping
  accumulation_steps: 1       # No gradient accumulation for now
  warmup_epochs: 10           # Longer warmup for stability
  ema_decay: 0.999            # EMA for better generation quality
  
  # Scheduler
  scheduler_type: cosine      # Cosine annealing
  min_lr_ratio: 0.01         # Don't go below 1% of base LR

# Loss configuration
loss:
  signal_loss_type: mse       # MSE for noise prediction
  signal_weight: 1.0
  use_huber: false           # Keep MSE for simplicity
  
# Evaluation during training
evaluation:
  eval_interval: 10          # Evaluate every 10 epochs
  num_eval_samples: 50       # Quick evaluation
  ddim_steps: 50             # DDIM steps for preview
  cfg_scale: 1.0             # No CFG during training evaluation

# Checkpointing
checkpoints:
  save_best: true
  save_last: true
  save_optimizer: true       # Save optimizer state for resuming
  save_ema: true            # Save EMA weights
  monitor_metric: val_noise  # Monitor validation noise loss
# Sequential Training Configuration for ABR Diffusion Model
# Implements curriculum learning to address multi-task conflicts

project:
  name: "ABR_Sequential_Training"
  description: "Sequential Training with Curriculum Learning"
  version: "1.0.0"

# Data Configuration
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4
  n_classes: 5
  
  splits:
    train_ratio: 0.75
    val_ratio: 0.15
    test_ratio: 0.10
    random_seed: 42
  
  dataloader:
    batch_size: 12         # Moderate batch size for stability
    num_workers: 2
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 1
    persistent_workers: true

# Model Architecture
model:
  type: "hierarchical_unet"  # or "simplified_abr" for the simplified version
  
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 64      # Moderate complexity
    n_levels: 3           # Simplified
    n_classes: 5
    dropout: 0.15
    
    encoder:
      n_s4_layers: 2
      d_state: 64
      use_enhanced_s4: true
      
    decoder:
      n_transformer_layers: 2
      n_heads: 8
      use_multi_scale_attention: true
      use_cross_attention: true
      
    outputs:
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration
diffusion:
  noise_schedule:
    type: "cosine"
    num_timesteps: 400     # Moderate complexity
    beta_start: 1e-4
    beta_end: 0.015
  
  sampling:
    type: "ddim"
    num_sampling_steps: 25
    temperature: 1.0
    clip_denoised: true

# Training Configuration
training:
  optimizer:
    type: "adamw"
    learning_rate: 5e-5    # Conservative learning rate
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  
  scheduler:
    type: "cosine_annealing"
    T_max: 50              # Per phase
    eta_min: 1e-8
  
  # These will be overridden by sequential training
  epochs: 150             # Total across all phases
  gradient_clip: 0.5
  accumulation_steps: 8
  log_frequency: 10
  
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 10
    save_best: true
    save_last: true
    save_dir: "checkpoints/sequential"
  
  early_stopping:
    patience: 30           # Will be overridden per phase
    min_delta: 1e-6
    monitor: "val_total_loss"
    mode: "min"

# SEQUENTIAL TRAINING CONFIGURATION
sequential_training:
  enabled: true
  
  phases:
    # Phase 1: Signal reconstruction only
    - phase: "signal_only"
      epochs: 25
      description: "Learn basic signal reconstruction and diffusion process"
      lr_multiplier: 1.0
      early_stopping_patience: 12
      enabled_tasks: ["diffusion"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 0.0
        threshold: 0.0
    
    # Phase 2: Signal + Classification
    - phase: "signal_classification"
      epochs: 35
      description: "Add classification while maintaining signal quality"
      lr_multiplier: 0.7     # Reduce LR for stability
      early_stopping_patience: 15
      enabled_tasks: ["diffusion", "classification"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.5    # High priority for classification
        threshold: 0.0
    
    # Phase 3: Signal + Classification + Threshold
    - phase: "signal_classification_threshold"
      epochs: 30
      description: "Add threshold regression"
      lr_multiplier: 0.5
      early_stopping_patience: 15
      enabled_tasks: ["diffusion", "classification", "threshold"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.0
        threshold: 1.0         # Add threshold
    
    # Phase 4: All tasks with careful peak integration
    - phase: "all_tasks_careful"
      epochs: 35
      description: "Carefully add peak detection with very low weights"
      lr_multiplier: 0.3
      early_stopping_patience: 20
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.15       # Very low
        peak_latency: 0.08     # Very low
        peak_amplitude: 0.02   # Extremely low
        classification: 1.8
        threshold: 0.8
    
    # Phase 5: Final balanced training
    - phase: "balanced_final"
      epochs: 25
      description: "Final balanced training of all tasks"
      lr_multiplier: 0.2
      early_stopping_patience: 15
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.2
        peak_latency: 0.1
        peak_amplitude: 0.03   # Still very low
        classification: 1.5
        threshold: 0.7

# Loss Configuration (base settings, overridden by phases)
loss:
  type: "abr_diffusion"
  peak_loss_type: "smooth_l1"
  huber_delta: 0.1
  
  # Base loss weights (will be overridden by sequential training phases)
  weights:
    diffusion: 1.0
    peak_exist: 0.1
    peak_latency: 0.05
    peak_amplitude: 0.001
    classification: 1.5
    threshold: 0.5
  
  # Enhanced class balance
  enhanced_class_balance: true
  class_weights: "balanced"
  focal_loss:
    use_focal: true
    alpha: 2.0
    gamma: 3.0

# Enhanced Monitoring
logging:
  level: "INFO"
  log_dir: "logs/sequential"
  use_tensorboard: true
  use_wandb: false
  enhanced_monitoring: true
  
  # Detailed sequential training tracking
  detailed_logging: true
  log_model_gradients: true
  log_loss_components: true
  log_phase_transitions: true

# Evaluation
evaluation:
  evaluation_type: "diffusion"
  metrics:
    signal_metrics: ["mse", "mae", "correlation", "snr"]
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1"]
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted"]
    threshold_metrics: ["threshold_mae", "threshold_r2"]
  
  output_dir: "outputs/sequential"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false

# Hardware
hardware:
  device: "auto"
  mixed_precision: true
  compile_model: false

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints/sequential"
  output_dir: "outputs/sequential"
  log_dir: "logs/sequential"
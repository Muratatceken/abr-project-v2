# ABR Hierarchical U-Net Configuration
# Denoising Diffusion Model for ABR Signal Generation

project:
  name: "ABR_HierarchicalUNet_Diffusion"
  description: "Hierarchical U-Net with S4 Encoder and Transformer Decoder for ABR Signal Generation"
  version: "1.0.0"
  author: "AI Assistant"

# Data Configuration
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4  # age, intensity, stimulus_rate, fmp
  n_classes: 5   # hearing loss types
  
  # Data splits (patient-stratified) - Optimized
  splits:
    train_ratio: 0.75    # Increased training data
    val_ratio: 0.15
    test_ratio: 0.10     # Reduced test set for more training data
    random_seed: 42
  
  # Data loading - CONVERGENCE OPTIMIZED
  dataloader:
    batch_size: 16       # REDUCED: Smaller batches for precise gradients
    num_workers: 4       # Reduced for stability
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 2   # Moderate prefetching
    persistent_workers: true

# Model Architecture
model:
  type: "hierarchical_unet"
  
  # Architecture parameters - Optimized
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 96      # Increased for more capacity
    n_levels: 4
    n_classes: 5
    dropout: 0.15          # Increased for better regularization
    
    # S4 Encoder settings - Enhanced
    encoder:
      n_s4_layers: 3       # Increased depth
      d_state: 96          # Increased capacity
      use_enhanced_s4: true
      
    # Transformer Decoder settings - Enhanced
    decoder:
      n_transformer_layers: 3  # Increased depth
      n_heads: 12             # Fixed: d_model (192) must be divisible by n_heads
      use_multi_scale_attention: true
      use_cross_attention: true
      
    # FiLM conditioning - Enhanced
    film:
      hidden_dim: 96       # Increased capacity
      dropout: 0.15        # Added dropout
      
    # Output heads - Enhanced
    outputs:
      signal_activation: "tanh"
      peak_head_dim: 256   # Increased capacity
      class_head_dim: 256  # Increased capacity
      threshold_head_dim: 256  # Increased capacity
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration
diffusion:
  # Noise schedule
  noise_schedule:
    type: "cosine"  # linear, cosine, quadratic
    num_timesteps: 1000
    peak_preserve_ratio: 0.3
    beta_start: 5e-5
    beta_end: 0.015
  
  # Sampling
  sampling:
    type: "ddpm"  # ddpm, ddim
    ddim_eta: 0.0
    num_sampling_steps: 50
    temperature: 1.0
    clip_denoised: true
    
    # Clinical constraints
    constraints:
      apply_constraints: true
      peak_latency_range: [1.0, 8.0]  # ms
      amplitude_range: [-0.5, 0.5]   # Î¼V
      smooth_kernel_size: 5

# Training Configuration
training:
  # Optimizer - CONVERGENCE FIXED
  optimizer:
    type: "adamw"
    learning_rate: 1e-5    # REDUCED: Fix plateau issue (was 2e-4)
    weight_decay: 1e-6     # REDUCED: Less regularization for gradual learning
    betas: [0.9, 0.999]    # Standard betas for stability
    eps: 1e-8
    amsgrad: false
  
  # Learning rate scheduler - IMPROVED DYNAMICS
  scheduler:
    type: "cosine_annealing_warm_restarts"
    T_0: 15               # Shorter cycles for more adaptation
    T_mult: 1             # No lengthening for consistent adaptation
    eta_min: 1e-8         # Much lower minimum for fine-tuning
    warmup_epochs: 3      # Shorter warmup for quicker adaptation
  
  # Training parameters - CONVERGENCE OPTIMIZED
  epochs: 100             # More reasonable for gradual learning
  gradient_clip: 0.3      # Stricter clipping for stability
  accumulation_steps: 8   # MORE accumulation for stable gradients
  log_frequency: 10       # More frequent monitoring
  
  # Validation and checkpointing
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 5      # More frequent saves
    save_best: true
    save_last: true
    save_dir: "checkpoints"
  
  # Early stopping - GRADUAL LEARNING OPTIMIZED
  early_stopping:
    patience: 25           # Moderate patience for gradual learning
    min_delta: 1e-6        # SMALLER threshold for gradual improvements
    monitor: "val_total_loss"
    mode: "min"

# Loss Configuration
loss:
  type: "abr_diffusion"
  
  # Loss weights - CONVERGENCE FIXED: Proper component balance
  weights:
    diffusion: 1.0
    peak_exist: 0.3        # REDUCED: More balanced
    peak_latency: 0.2      # REDUCED: Prevent dominance
    peak_amplitude: 0.01   # KEEP LOW: Was causing massive scale issues
    classification: 0.8    # Moderate importance
    threshold: 0.05        # HEAVILY REDUCED: Was causing plateau at ~70
  
  # Loss settings - Optimized
  peak_loss_type: "smooth_l1"  # Better than huber for peaks
  huber_delta: 0.5             # Reduced delta
  
  # Class imbalance handling - Enhanced
  class_weights: "balanced"
  focal_loss:
    use_focal: true        # Enabled for better class balance
    alpha: 0.75            # Optimized alpha
    gamma: 2.5             # Increased gamma
  
  # Perceptual loss
  perceptual:
    use_perceptual: false
    feature_weights:
      peak_preservation: 2.0
      morphology: 1.0
      spectral: 0.5
      temporal: 1.0

# Evaluation Configuration
evaluation:
  metrics:
    # Signal quality metrics
    signal_metrics: ["mse", "mae", "correlation", "dtw_distance"]
    
    # Peak prediction metrics
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1"]
    
    # Classification metrics
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted", "confusion_matrix"]
    
    # Threshold metrics
    threshold_metrics: ["threshold_mae", "threshold_r2"]
  
  # Visualization
  visualization:
    save_plots: true
    plot_types: ["reconstruction", "peaks", "latent_space", "generation_samples"]
    n_samples_plot: 10
    
  # Output
  output_dir: "outputs/evaluation"
  save_detailed_results: true

# Inference Configuration
inference:
  # Generation parameters
  generation:
    num_samples: 100
    batch_size: 16
    temperature: 1.0
    guidance_scale: 1.0
    
  # Conditional generation
  conditioning:
    age_range: [20, 80]
    intensity_range: [60, 100]
    rate_range: [10, 80]
    fmp_range: [0.5, 1.0]
    
  # Output
  output_dir: "outputs/inference"
  save_formats: ["signals", "peaks", "predictions", "metadata"]

# Logging and Monitoring
logging:
  level: "INFO"
  log_dir: "logs"
  use_tensorboard: true
  use_wandb: true          # Enabled for better experiment tracking
  
  # Weights & Biases configuration (if enabled)
  wandb:
    project: "abr-hierarchical-unet"
    entity: null
    tags: ["diffusion", "abr", "s4", "transformer"]

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true      # Enabled for reproducible results
  benchmark: false         # Disabled for deterministic behavior

# Hardware Configuration
hardware:
  device: "auto"  # auto, cuda, cpu
  gpu_id: 0
  mixed_precision: true
  compile_model: true   # Enabled PyTorch 2.0 compilation for speed

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints"
  output_dir: "outputs"
  log_dir: "logs" 
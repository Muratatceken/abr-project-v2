# Full enhanced model with all architectural improvements
# Tests combined impact of all enhancements

# Basic experiment settings
seed: 42
device: "cuda"  # Will fallback to CPU if CUDA unavailable

# Dataset configuration
data:
  train_csv: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  val_csv: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  sequence_length: 200
  static_order: ["Age", "Intensity", "Stimulus Rate", "FMP"]
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

# DataLoader configuration
loader:
  batch_size: 32
  num_workers: 4
  pin_memory: true
  drop_last: true
  prefetch_factor: 2
  persistent_workers: false
  use_peak_balanced_sampler: true
  peak_sampling_strategy: 'weighted'

# Model architecture
model:
  input_channels: 1
  static_dim: 4
  sequence_length: 200
  d_model: 256
  n_layers: 6
  n_heads: 8
  ff_mult: 4
  dropout: 0.1
  use_timestep_cond: true
  use_static_film: true
  use_cross_attention: true
  joint_static_generation: true
  use_learned_pos_emb: true
  film_residual: true
  use_multi_scale_fusion: true
  use_advanced_blocks: true
  ablation_mode: "full"

ablation:
  use_cross_attention: true
  cross_attention_heads: 4
  cross_attention_dropout: 0.1
  
  joint_static_generation: true
  static_recon_weight: 0.1
  
  use_learned_pos_emb: true
  pos_emb_dropout: 0.1
  
  film_residual: true
  
  use_multi_scale_fusion: true
  fusion_scales: [1, 3, 5, 7]
  
  use_advanced_blocks: true
  use_multi_scale_attention: true
  use_gated_ffn: true
  attention_dropout: 0.1
  
  ablation_mode: "full"

exp_name: "abr_full_enhanced"
log_dir: "runs/ablation/full_enhanced"

# Enable all multi-task components
multi_task:
  enabled: true
  loss_weights:
    signal: 1.0
    peak_classification: 0.5
    static_reconstruction: 0.1
  task_lr_multipliers:
    signal: 1.0
    peak_classification: 0.8
    static_reconstruction: 1.2

# Diffusion configuration
diffusion:
  num_train_steps: 1000
  schedule: "cosine"
  v_prediction: true
  sample_steps: 60
  ddim_eta: 0.0
  cfg_scale: 1.0

# Training configuration
trainer:
  max_epochs: 120  # More epochs for complex model
  log_every_steps: 25  # More frequent logging
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: 32
  check_val_every_n_epoch: 5
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  ckpt_dir: "checkpoints/enhanced"
  validate_every_epochs: 1
  sample_every_epochs: 2
  num_val_plots: 8
  num_sample_plots: 8
  resume: false
  save_every_epochs: 5

# Optimizer configuration
optim:
  name: "adamw"
  lr: 0.0001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 0.00000001
  amp: false
  ema_decay: 0.9999
  grad_clip: 1.0

# Scheduler configuration
scheduler:
  name: "cosine_with_warmup"
  warmup_steps: 1000
  max_steps: 50000
  eta_min: 1e-6

# Loss configuration
loss:
  stft_weight: 0.15
  stft:
    n_fft: 64
    hop_length: 16
    win_length: 64
    eps: 0.00000001

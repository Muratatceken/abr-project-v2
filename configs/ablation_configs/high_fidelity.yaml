# High-fidelity signal reconstruction with joint generation
# Optimized for maximum signal similarity and peak detection

# Data configuration
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

# DataLoader configuration
loader:
  batch_size: 32
  num_workers: 4
  pin_memory: true
  drop_last: true
  prefetch_factor: 2
  persistent_workers: false
  use_peak_balanced_sampler: true
  peak_sampling_strategy: 'weighted'

# Model architecture - Full enhanced
model:
  input_channels: 1
  static_dim: 4
  sequence_length: 200
  d_model: 256
  n_layers: 6
  n_heads: 8
  ff_mult: 4
  dropout: 0.1
  use_timestep_cond: true
  use_cross_attention: true
  use_learned_pos_emb: true
  film_residual: true
  use_multi_scale_fusion: true
  use_advanced_blocks: true

# Multi-task configuration - Optimized for joint generation
multi_task:
  enabled: true
  peak_classification_head: true
  static_reconstruction_head: true
  loss_weights:
    signal: 1.0
    peak_classification: 0.8  # Higher weight for peak accuracy
    static_reconstruction: 0.2  # Higher weight for joint generation
  validation_metric: "combined"
  validation_weights:
    signal: 0.5
    peak_classification: 0.5

# Diffusion configuration
diffusion:
  num_train_steps: 1000
  schedule: "cosine"
  v_prediction: true
  sample_steps: 60
  ddim_eta: 0.0
  cfg_scale: 1.0

# Trainer configuration
trainer:
  max_epochs: 120
  log_every_steps: 50
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: 32
  check_val_every_n_epoch: 5
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  ckpt_dir: "checkpoints/high_fidelity"
  validate_every_epochs: 1
  sample_every_epochs: 2
  num_val_plots: 8
  num_sample_plots: 8
  resume: false
  save_every_epochs: 5
  keep_last_k: 5

# Early stopping configuration - More patient for convergence
early_stopping:
  enabled: true
  patience: 30  # Increased patience for better convergence
  metric: "val_selection"
  mode: "min"
  min_delta: 0.0000001  # Tighter convergence criterion

# Visualization configuration
viz:
  plot_spectrogram: true
  plot_audio: false
  audio_sample_rate: 20000
  max_waveform_plots: 8
  max_spectrogram_plots: 4
  figsize: [12, 8]

# Optimizer configuration - Conservative for stability
optim:
  name: "adamw"
  lr: 0.00008  # Slightly lower for stability
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 0.00000001
  amp: false
  ema_decay: 0.9999
  grad_clip: 1.0

# Scheduler configuration
scheduler:
  name: "cosine_with_warmup"
  warmup_steps: 1000
  max_steps: 50000
  eta_min: 1e-6

# Loss configuration - Optimized for reconstruction quality
loss:
  stft_weight: 0.3  # Higher STFT weight for frequency-domain accuracy
  stft:
    n_fft: 64
    hop_length: 16
    win_length: 64
    eps: 0.00000001

# Advanced features for quality
curriculum_learning:
  enabled: true
  start_epoch: 5
  ramp_epochs: 15
  difficulty_metric: "peak_clarity"

focal_loss:
  enabled: true
  alpha: 2.0
  gamma: 2.0

advanced_augmentation:
  enabled: true
  mixup_alpha: 0.2
  cutmix_alpha: 0.5
  peak_jitter_std: 0.02
  preserve_peaks: true

# Monitoring
monitoring:
  enabled: true
  save_dir: "monitoring/high_fidelity"
  log_gradients: true
  log_parameters: true
  track_memory: true

exp_name: "abr_high_fidelity_reconstruction"
log_dir: "runs/high_fidelity"
seed: 42

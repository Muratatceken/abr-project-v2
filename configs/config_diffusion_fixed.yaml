# ABR Diffusion Model Configuration - CRITICAL FIXES APPLIED
# Fixed configuration addressing all identified issues

project:
  name: "ABR_Diffusion_Fixed"
  description: "Fixed ABR Diffusion Model with Proper Multi-Task Learning"
  version: "2.0.0"
  author: "AI Assistant - Fixed Version"

# Data Configuration
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4
  n_classes: 5
  
  # Optimized data splits
  splits:
    train_ratio: 0.75
    val_ratio: 0.15
    test_ratio: 0.10
    random_seed: 42
  
  # Optimized data loading for stable training
  dataloader:
    batch_size: 8          # SMALLER: More stable gradients for complex diffusion model
    num_workers: 2         # Reduced for stability
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 1
    persistent_workers: true

# Model Architecture - OPTIMIZED
model:
  type: "hierarchical_unet"
  
  # Simplified architecture for better convergence
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 64      # REDUCED: Prevent overcomplication
    n_levels: 3           # REDUCED: Simpler architecture
    n_classes: 5
    dropout: 0.2          # INCREASED: Better regularization
    
    # Simplified S4 Encoder
    encoder:
      n_s4_layers: 2       # REDUCED: Simpler
      d_state: 64          # REDUCED: Prevent overfit
      use_enhanced_s4: true
      
    # Simplified Transformer Decoder
    decoder:
      n_transformer_layers: 2  # REDUCED: Simpler
      n_heads: 8              # REDUCED: More manageable
      use_multi_scale_attention: true
      use_cross_attention: true
      
    # Simplified FiLM conditioning
    film:
      hidden_dim: 64       # REDUCED: Simpler
      dropout: 0.2         # Increased regularization
      
    # Simplified output heads
    outputs:
      signal_activation: "tanh"
      peak_head_dim: 128   # REDUCED: Simpler
      class_head_dim: 128  # REDUCED: Simpler  
      threshold_head_dim: 128  # REDUCED: Simpler
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration - OPTIMIZED
diffusion:
  noise_schedule:
    type: "cosine"
    num_timesteps: 500     # REDUCED: Faster training/inference
    peak_preserve_ratio: 0.3
    beta_start: 1e-4
    beta_end: 0.015
  
  sampling:
    type: "ddim"
    ddim_eta: 0.0
    num_sampling_steps: 25  # REDUCED: Faster inference
    temperature: 1.0
    clip_denoised: true
    
    constraints:
      apply_constraints: true
      peak_latency_range: [1.0, 8.0]
      amplitude_range: [-0.5, 0.5]
      smooth_kernel_size: 5

# Training Configuration - CRITICAL FIXES
training:
  # Conservative optimizer settings
  optimizer:
    type: "adamw"
    learning_rate: 5e-6    # MUCH LOWER: Prevent instability
    weight_decay: 1e-7     # LOWER: Less aggressive regularization
    betas: [0.9, 0.999]
    eps: 1e-8
    amsgrad: false
  
  # Enhanced learning rate scheduling
  scheduler:
    type: "cosine_annealing_warm_restarts"
    T_0: 20               # Longer cycles for stability
    T_mult: 1
    eta_min: 1e-9         # Very low minimum
    warmup_epochs: 5      # Longer warmup
  
  # Stable training parameters
  epochs: 200             # More epochs for gradual learning
  gradient_clip: 0.5      # Moderate clipping
  accumulation_steps: 16  # LARGE: Very stable gradients
  log_frequency: 5        # Frequent monitoring
  
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 10
    save_best: true
    save_last: true
    save_dir: "checkpoints/diffusion_fixed"
  
  # Patient early stopping
  early_stopping:
    patience: 40           # Very patient for gradual learning
    min_delta: 1e-7        # Very small improvements count
    monitor: "val_total_loss"
    mode: "min"

# Loss Configuration - CRITICAL REBALANCING
loss:
  type: "abr_diffusion"
  
  # FIXED LOSS WEIGHTS - Addresses catastrophic imbalances
  weights:
    diffusion: 1.0         # Diffusion/signal reconstruction
    peak_exist: 0.1        # HEAVILY REDUCED: Was causing instability
    peak_latency: 0.05     # HEAVILY REDUCED: Prevent dominance
    peak_amplitude: 0.001  # CRITICAL: Prevent million-scale issues
    classification: 2.0    # INCREASED: Priority for classification
    threshold: 0.5         # MODERATE: Important but controlled
  
  # Enhanced loss settings
  peak_loss_type: "huber"  # Robust loss
  huber_delta: 0.1         # Small delta for stability
  
  # ENHANCED class imbalance handling
  class_weights: "balanced"
  focal_loss:
    use_focal: true
    alpha: 2.0             # INCREASED: Better minority class focus
    gamma: 3.0             # INCREASED: More aggressive focusing
  
  # Enhanced perceptual loss
  perceptual:
    use_perceptual: false   # Disabled to reduce complexity

# Evaluation Configuration
evaluation:
  # Comprehensive evaluation type
  evaluation_type: "diffusion"  # Use proper diffusion evaluation
  
  metrics:
    signal_metrics: ["mse", "mae", "correlation", "dtw_distance", "snr"]
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1"]
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted", "confusion_matrix"]
    threshold_metrics: ["threshold_mae", "threshold_r2"]
  
  visualization:
    save_plots: true
    plot_types: ["reconstruction", "peaks", "classification", "generation_samples"]
    n_samples_plot: 10
    
  output_dir: "outputs/evaluation_fixed"
  save_detailed_results: true

# Inference Configuration
inference:
  generation:
    num_samples: 50        # Reasonable for testing
    batch_size: 4          # Small for stability
    temperature: 1.0
    guidance_scale: 1.0
    
  conditioning:
    age_range: [20, 80]
    intensity_range: [60, 100]
    rate_range: [10, 80]
    fmp_range: [0.5, 1.0]
    
  output_dir: "outputs/inference_fixed"
  save_formats: ["signals", "peaks", "predictions", "metadata"]

# Enhanced Logging and Monitoring
logging:
  level: "INFO"
  log_dir: "logs/diffusion_fixed"
  use_tensorboard: true
  use_wandb: false        # Disabled for local testing
  
  # Enhanced monitoring
  detailed_logging: true
  log_model_gradients: true
  log_loss_components: true
  log_peak_statistics: true

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false

# Hardware Configuration
hardware:
  device: "auto"
  gpu_id: 0
  mixed_precision: true   # Can help with stability
  compile_model: false    # Disabled for debugging

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints/diffusion_fixed"
  output_dir: "outputs/diffusion_fixed"
  log_dir: "logs/diffusion_fixed"

# SEQUENTIAL TRAINING STRATEGY
sequential_training:
  enabled: false          # Can be enabled for curriculum learning
  phases:
    - phase: "signal_only"
      epochs: 20
      loss_weights:
        diffusion: 1.0
        classification: 0.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        threshold: 0.0
    
    - phase: "signal_classification"
      epochs: 30
      loss_weights:
        diffusion: 1.0
        classification: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        threshold: 0.0
    
    - phase: "all_tasks"
      epochs: 150
      loss_weights:
        diffusion: 1.0
        classification: 2.0
        peak_exist: 0.1
        peak_latency: 0.05
        peak_amplitude: 0.001
        threshold: 0.5
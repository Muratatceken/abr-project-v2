# Fast Sequential Training Configuration - Based on Observed Rapid Convergence
# Optimized for L4 GPU with much shorter training based on excellent loss convergence

project:
  name: "ABR_Sequential_Training_Fast"
  description: "Fast Sequential Training - Rapid Convergence Optimized"
  version: "1.0.0"

# Data Configuration - OPTIMIZED FOR L4 GPU
data:
  dataset_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  signal_length: 200
  static_dim: 4
  n_classes: 5
  
  splits:
    train_ratio: 0.75
    val_ratio: 0.15
    test_ratio: 0.10
    random_seed: 42
  
  # OPTIMIZED for L4 GPU
  dataloader:
    batch_size: 64          # Large batches for L4
    num_workers: 4
    pin_memory: true
    drop_last: true
    shuffle_train: true
    prefetch_factor: 4
    persistent_workers: true

# Model Architecture
model:
  type: "hierarchical_unet"
  
  architecture:
    signal_length: 200
    static_dim: 4
    base_channels: 64
    n_levels: 3
    n_classes: 5
    dropout: 0.15
    
    encoder:
      n_s4_layers: 2
      d_state: 64
      use_enhanced_s4: true
      
    decoder:
      n_transformer_layers: 2
      n_heads: 8
      use_multi_scale_attention: true
      use_cross_attention: true
      
    outputs:
      use_attention_heads: true
      predict_uncertainty: true

# Diffusion Configuration
diffusion:
  noise_schedule:
    type: "cosine"
    num_timesteps: 400
    beta_start: 1e-4
    beta_end: 0.015
  
  sampling:
    type: "ddim"
    num_sampling_steps: 25
    temperature: 1.0
    clip_denoised: true

# Training Configuration - FAST CONVERGENCE
training:
  optimizer:
    type: "adamw"
    learning_rate: 1e-4     # Higher LR for larger batches
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  
  scheduler:
    type: "cosine_annealing"
    T_max: 15               # Much shorter cycles
    eta_min: 1e-8
  
  epochs: 40              # MUCH SHORTER: Based on fast convergence
  gradient_clip: 1.0
  accumulation_steps: 2
  log_frequency: 5
  
  validation:
    frequency: 1
    compute_metrics: true
  
  checkpointing:
    save_frequency: 3       # Frequent saves for short training
    save_best: true
    save_last: true
    save_dir: "checkpoints/sequential_fast"
  
  early_stopping:
    patience: 8             # Much shorter patience
    min_delta: 1e-6
    monitor: "val_total_loss"
    mode: "min"

# FAST SEQUENTIAL TRAINING - Based on observed rapid convergence
sequential_training:
  enabled: true
  
  phases:
    # Phase 1: Signal only - VERY FAST (loss drops from 0.67 to 0.12 in 0.5 epochs!)
    - phase: "signal_only"
      epochs: 5             # VERY SHORT: Already converging rapidly
      description: "Learn basic signal reconstruction and diffusion process"
      lr_multiplier: 1.0
      early_stopping_patience: 3
      enabled_tasks: ["diffusion"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 0.0
        threshold: 0.0
    
    # Phase 2: Signal + Classification - FAST
    - phase: "signal_classification"
      epochs: 10            # SHORT: Should converge quickly
      description: "Add classification while maintaining signal quality"
      lr_multiplier: 0.8
      early_stopping_patience: 5
      enabled_tasks: ["diffusion", "classification"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.5
        threshold: 0.0
    
    # Phase 3: Signal + Classification + Threshold
    - phase: "signal_classification_threshold"
      epochs: 8
      description: "Add threshold regression"
      lr_multiplier: 0.6
      early_stopping_patience: 4
      enabled_tasks: ["diffusion", "classification", "threshold"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.0
        peak_latency: 0.0
        peak_amplitude: 0.0
        classification: 2.0
        threshold: 1.0
    
    # Phase 4: Careful peak integration
    - phase: "all_tasks_careful"
      epochs: 10
      description: "Carefully add peak detection with very low weights"
      lr_multiplier: 0.4
      early_stopping_patience: 5
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.15
        peak_latency: 0.08
        peak_amplitude: 0.02
        classification: 1.8
        threshold: 0.8
    
    # Phase 5: Final balanced training
    - phase: "balanced_final"
      epochs: 7
      description: "Final balanced training of all tasks"
      lr_multiplier: 0.3
      early_stopping_patience: 4
      enabled_tasks: ["diffusion", "classification", "threshold", "peaks"]
      loss_weights:
        diffusion: 1.0
        peak_exist: 0.2
        peak_latency: 0.1
        peak_amplitude: 0.03
        classification: 1.5
        threshold: 0.7

# Loss Configuration
loss:
  type: "abr_diffusion"
  peak_loss_type: "smooth_l1"
  huber_delta: 0.1
  
  # Base loss weights
  weights:
    diffusion: 1.0
    peak_exist: 0.1
    peak_latency: 0.05
    peak_amplitude: 0.001
    classification: 1.5
    threshold: 0.5
  
  # Enhanced class balance
  enhanced_class_balance: true
  class_weights: "balanced"
  focal_loss:
    use_focal: true
    alpha: 2.0
    gamma: 3.0

# Enhanced Monitoring
logging:
  level: "INFO"
  log_dir: "logs/sequential_fast"
  use_tensorboard: true
  use_wandb: false
  enhanced_monitoring: true
  
  detailed_logging: true
  log_model_gradients: false
  log_loss_components: true
  log_phase_transitions: true

# Evaluation
evaluation:
  evaluation_type: "diffusion"
  metrics:
    signal_metrics: ["mse", "mae", "correlation", "snr"]
    peak_metrics: ["peak_mae", "peak_accuracy", "existence_f1"]
    classification_metrics: ["accuracy", "f1_macro", "f1_weighted"]
    threshold_metrics: ["threshold_mae", "threshold_r2"]
  
  output_dir: "outputs/sequential_fast"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: false
  benchmark: true           # Enable cuDNN benchmarking for speed

# Hardware - OPTIMIZED for L4
hardware:
  device: "cuda"
  mixed_precision: true
  compile_model: true       # PyTorch 2.0 compilation for speed

# Paths
paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints/sequential_fast"
  output_dir: "outputs/sequential_fast" 
  log_dir: "logs/sequential_fast"
# Optimized ABR Diffusion Training Configuration
# Addresses architectural mismatches and improves training stability

training:
  device: cuda                 # Auto-detect CUDA/CPU
  mixed_precision: true        # Enable AMP for speed and memory efficiency
  random_seed: 42
  early_stopping_patience: 30  # Increased patience for better convergence

logging:
  checkpoint_dir: checkpoints/enhanced
  log_dir: logs/enhanced
  use_tensorboard: true
  use_wandb: false
  wandb_project: abr-diffusion-enhanced
  wandb_run_name: abr_enhanced_training
  log_interval: 25             # More frequent logging
  val_interval: 1              # Validate every epoch
  val_preview_samples: 8       # Generate preview samples during validation
  save_interval: 5             # Save checkpoint every 5 epochs

data:
  path: data/processed/ultimate_dataset_with_clinical_thresholds.pkl
  batch_size: 128              # Reduced for better generalization
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  num_workers: 8               # Parallel data loading
  pin_memory: true
  persistent_workers: true     # Reuse workers for efficiency
  prefetch_factor: 4           # Prefetch more batches

model:
  # Core architecture
  signal_length: 200
  static_dim: 4
  input_channels: 1
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  
  # S4 configuration
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Transformer configuration
  num_transformer_layers: 2
  num_heads: 8
  use_multi_scale_attention: true
  use_cross_attention: true
  
  # Regularization
  dropout: 0.1
  film_dropout: 0.15
  
  # Enhanced architecture features
  use_enhanced_model: true     # Use EnhancedHierarchicalUNet
  timestep_dim: 128           # Timestep embedding dimension
  context_dim: 256            # Global context dimension
  use_dual_branch: true       # Envelope + detail branches
  use_global_context: true    # Global context conditioning
  use_v_prediction: true      # V-prediction instead of noise
  
  # Architecture flags (CRITICAL - must match training)
  use_cfg: true
  use_attention_heads: true
  predict_uncertainty: false    # Disable for simplicity
  enable_joint_generation: false # Focus on signal generation only
  use_task_specific_extractors: true
  use_attention_skip_connections: true
  channel_multiplier: 2.0
  
  # Output configuration
  use_robust_heads: true
  
  # Data augmentation
  use_augmentation: true
  augmentation:
    time_shift_samples: 2
    noise_std: 0.01
    apply_prob: 0.3

diffusion:
  schedule_type: cosine        # Cosine schedule for better sampling
  num_timesteps: 1000
  clip_denoised: false        # Disable clipping for better dynamics
  eta: 0.0                    # Deterministic DDIM sampling
  prediction_type: v_prediction # Use v-prediction for better convergence
  use_p2_weighting: true      # P2 loss weighting for stability
  p2_k: 1.0                   # P2 weighting parameter k
  p2_gamma: 1.0               # P2 weighting parameter gamma

optimization:
  epochs: 150                  # Sufficient for convergence
  learning_rate: 0.0001       # Conservative LR for stability
  weight_decay: 0.0001
  grad_clip_norm: 1.0         # Gradient clipping
  accumulation_steps: 1       # No gradient accumulation for now
  warmup_epochs: 10           # Longer warmup for stability
  ema_decay: 0.999            # EMA for better generation quality
  
  # Scheduler
  scheduler_type: cosine      # Cosine annealing
  min_lr_ratio: 0.01         # Don't go below 1% of base LR

# Loss configuration
loss:
  signal_loss_type: mse       # MSE for noise prediction
  signal_weight: 1.0
  use_huber: false           # Keep MSE for simplicity
  
# Evaluation during training
evaluation:
  eval_interval: 10          # Evaluate every 10 epochs
  num_eval_samples: 50       # Quick evaluation
  ddim_steps: 100            # More DDIM steps for better preview quality
  cfg_scale: 1.0             # No CFG during training evaluation

# Checkpointing
checkpoints:
  save_best: true
  save_last: true
  save_optimizer: true       # Save optimizer state for resuming
  save_ema: true            # Save EMA weights
  monitor_metric: val_noise  # Monitor validation noise loss
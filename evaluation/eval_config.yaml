# Comprehensive ABR Model Evaluation Configuration
# Example configuration for detailed model assessment

# ============== GENERAL SETTINGS ==============
batch_size: 32
save_dir: "outputs/evaluation"

# ============== RECONSTRUCTION QUALITY ==============
reconstruction:
  # Enable advanced metrics
  dtw: true              # Dynamic Time Warping (requires fastdtw)
  fft_mse: true          # Spectral analysis via FFT
  correlation_analysis: true
  
  # Signal quality thresholds
  quality_thresholds:
    excellent_snr: 20.0   # dB
    good_snr: 15.0        # dB
    acceptable_snr: 10.0  # dB
    poor_snr: 5.0         # dB

# ============== PEAK ESTIMATION ==============
peaks:
  # Evaluation metrics
  existence_metrics: ["accuracy", "f1", "auc"]
  regression_metrics: ["mae", "rmse", "r2"]
  
  # Clinical tolerances
  latency_tolerance: 0.5    # ms - acceptable error
  amplitude_tolerance: 0.1  # μV - acceptable error
  
  # Error analysis
  error_percentiles: [25, 50, 75, 90, 95]

# ============== CLASSIFICATION ==============
classification:
  # Metrics to compute
  metrics: 
    - "accuracy"
    - "balanced_accuracy"
    - "f1_macro"
    - "f1_micro"
    - "f1_weighted"
    - "confusion_matrix"
    - "classification_report"
  
  # Per-class analysis
  per_class_metrics: true
  
  # Class distribution analysis
  distribution_analysis: true

# ============== THRESHOLD ESTIMATION ==============
threshold:
  # Regression metrics
  metrics: ["mae", "mse", "rmse", "r2", "log_mae"]
  
  # Clinical thresholds
  clinical_ranges:
    normal: [0, 25]        # dB SPL
    mild: [26, 40]         # dB SPL
    moderate: [41, 55]     # dB SPL
    moderate_severe: [56, 70]  # dB SPL
    severe: [71, 90]       # dB SPL
    profound: [91, 120]    # dB SPL
  
  # Error analysis
  error_analysis:
    percentiles: [10, 25, 50, 75, 90]
    absolute_error: true
    relative_error: true

# ============== CLINICAL FAILURE FLAGS ==============
clinical:
  # Failure mode thresholds
  thresholds:
    threshold_overestimate: 15.0  # dB - clinical significance
    threshold_underestimate: 15.0 # dB - clinical significance
    peak_latency_tolerance: 0.5   # ms - acceptable peak timing error
    peak_amplitude_tolerance: 0.1 # μV - acceptable amplitude error
  
  # Severe case definitions
  severe_classes: [3, 4]  # TOTAL, İTİK (indices in class list)
  normal_class: 0         # NORMAL (index in class list)
  
  # Clinical significance flags
  flags:
    false_peak_detection: true
    missed_peak_detection: true
    threshold_overestimation: true
    threshold_underestimation: true
    severe_class_mismatch: true
    normal_misclassified_as_severe: true

# ============== BOOTSTRAP CONFIDENCE INTERVALS ==============
bootstrap:
  enabled: true
  n_samples: 500
  ci_percentile: 95

# ============== ENHANCED VISUALIZATION ==============
visualization:
  # General settings
  figsize: [15, 10]
  dpi: 150
  save_format: "png"
  
  # Diagnostic plots
  waveform_samples: 5     # Number of sample waveforms to plot
  
  # Plot types to generate
  plots:
    signal_reconstruction: true
    peak_predictions: true
    classification_matrix: true
    threshold_scatter: true
    error_distributions: true
    clinical_overlays: true      # Enable clinical range overlays
    diagnostic_cards: true       # Generate multi-panel diagnostic cards
    quantile_analysis: true      # Per-class and per-range error analysis
    
  # Color schemes
  colors:
    true_signal: "blue"
    pred_signal: "red"
    true_peak: "green"
    pred_peak: "red"
    perfect_line: "gray"
    
  # Clinical overlays
  clinical_overlays:
    show_patient_id: true
    show_class_info: true
    show_threshold_info: true
    peak_markers: true
    
  # Diagnostic cards
  diagnostic_cards:
    layout: "2x2"              # Grid layout for multi-panel cards
    include_text_overlay: true  # Add textual information
    card_figsize: [12, 10]     # Size for diagnostic cards

# ============== STATISTICAL ANALYSIS ==============
statistics:
  # Confidence intervals
  confidence_level: 0.95
  
  # Bootstrap analysis
  bootstrap:
    enabled: false
    n_samples: 1000
    
  # Significance testing
  significance_tests:
    enabled: false
    alpha: 0.05

# ============== OUTPUT SETTINGS ==============
output:
  # File formats
  save_formats:
    metrics: ["json", "csv"]
    plots: ["png", "pdf"]
    
  # Detailed outputs
  save_batch_results: true
  save_individual_predictions: false
  save_error_analysis: true
  
  # Compression
  compress_results: false

# ============== LOGGING ==============
logging:
  # Console output
  verbose: true
  progress_bar: true
  
  # External logging
  tensorboard:
    enabled: false
    log_dir: "outputs/evaluation/tensorboard"
    
  wandb:
    enabled: false
    project: "abr-evaluation"
    tags: ["comprehensive", "evaluation"]

# ============== PERFORMANCE ==============
performance:
  # Memory management
  batch_processing: true
  clear_cache: true
  
  # Parallel processing
  num_workers: 0  # Single process for evaluation stability
  
  # GPU settings
  mixed_precision: false  # Disable for evaluation accuracy 
# Optimized ABR Architecture Configuration
# For OptimizedHierarchicalUNet with enhanced features

experiment:
  name: "abr_optimized_clinical_thresholds"
  description: "Ultimate training with OptimizedHierarchicalUNet using clinical thresholds dataset for maximum clinical applicability"
  random_seed: 42
  tags: ["optimized-architecture", "clinical-thresholds", "multi-scale-attention", "task-specific-extractors", "joint-generation", "ultimate-results"]

# Data Configuration
data:
  data_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  valid_peaks_only: false
  val_split: 0.2
  augment: true
  cfg_dropout_prob: 0.1
  normalize_signal: true
  normalize_static: true

# Model Configuration (OptimizedHierarchicalUNet)
model:
  type: "optimized_hierarchical_unet_v2"
  input_channels: 1
  static_dim: 4
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  signal_length: 200
  num_classes: 5
  
  # S4 Configuration
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Transformer Configuration (optimized)
  num_transformer_layers: 2  # Reduced from 3 for efficiency
  num_heads: 8
  use_multi_scale_attention: true
  use_cross_attention: true
  
  # FiLM and Conditioning
  dropout: 0.1
  film_dropout: 0.15
  use_cfg: true
  
  # Output Configuration
  use_attention_heads: true
  predict_uncertainty: true
  
  # Joint Generation
  enable_joint_generation: true
  static_param_ranges:
    age: [-2.0, 2.0]
    intensity: [-2.0, 2.0]
    stimulus_rate: [-2.0, 2.0]
    fmp: [0.0, 150.0]
  
  # Optimization Features
  use_task_specific_extractors: true
  use_attention_skip_connections: true
  channel_multiplier: 2.0

# Training Configuration
training:
  batch_size: 8  # Reduced for CPU training
  num_epochs: 50  # Reduced for faster testing
  learning_rate: 0.0003  # Optimized for new architecture
  weight_decay: 1e-4
  use_amp: false  # Disabled for CPU
  patience: 15
  gradient_clip_norm: 1.0
  gradient_accumulation_steps: 4  # Increased to maintain effective batch size
  num_workers: 2  # Reduced for CPU
  
  # Task-specific learning rate multipliers
  task_lr_multipliers:
    task_extractors: 1.2
    attention_layers: 0.8
    s4_layers: 1.0
    other: 1.0

# Loss Configuration (Multi-task with clinical thresholds)
loss:
  use_focal_loss: true
  use_class_weights: true
  peak_loss_type: "mse"
  use_log_threshold: false  # Disabled for clinical thresholds (already in dB HL)
  use_uncertainty_threshold: true  # Enable uncertainty for clinical reliability
  enable_static_param_loss: true
  
  # Loss weights (optimized for clinical threshold learning)
  loss_weights:
    signal: 1.0
    peak_exist: 0.5
    peak_latency: 2.0  # Increased for multi-scale attention
    peak_amplitude: 2.0
    classification: 2.5  # Slightly reduced to emphasize thresholds
    threshold: 3.0  # Increased for clinical threshold importance
    joint_generation: 0.8  # Increased for better joint learning

# Optimizer Configuration
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8
  amsgrad: false

# Scheduler Configuration
scheduler:
  type: "cosine_with_warmup"
  warmup_steps: 1000
  eta_min: 1e-6
  T_max: 100  # Same as num_epochs

# CFG Configuration
cfg:
  joint_cfg:
    enabled: true
    guidance_scale: 7.5
    dropout_prob: 0.1

# Hardware Configuration
hardware:
  compile_model: true  # PyTorch 2.0 compilation
  use_flash_attention: true  # Memory efficient attention
  memory_fraction: 0.9
  benchmark: true
  deterministic: false

# Validation Configuration
validation:
  fast_mode: true  # Skip generation during regular validation
  full_validation_every: 10  # Do full validation less frequently
  ddim_steps: 20  # Reduce DDIM steps when generation is needed
  skip_generation: true  # Skip generation for speed
  use_cv: false
  cv_folds: 5
  cv_strategy: "StratifiedGroupKFold"

# Logging Configuration
logging:
  output_dir: null  # Auto-generated
  use_wandb: false
  wandb_project: "abr-optimized-v2"
  use_tensorboard: true
  log_every: 10
  save_every: 10
  save_formats: ["pytorch"]  # Can add "onnx" for deployment

# Evaluation Configuration
evaluation:
  architecture_evaluation: true
  compute_metrics: true
  save_predictions: true
  create_visualizations: true

# Output Configuration
output:
  save_attention_maps: true
  save_feature_maps: false
  save_generated_samples: true
  num_sample_generations: 10

# Curriculum Learning (Optional)
curriculum:
  enabled: false
  ramp_epochs: 5
  peak_start: 5
  class_start: 3
  threshold_start: 10

# Advanced Settings
advanced:
  memory_efficient_attention: true
  gradient_checkpointing: false
  mixed_precision_level: "O1"
  distributed_training: false
  
  # Model-specific optimizations
  use_fused_layernorm: true
  use_fused_adam: true
  
  # Debugging
  detect_anomaly: false
  profile_memory: false 
# ABR Production Training Configuration - IMPROVED VERSION
# Enhanced training setup addressing architectural issues and performance problems

# ============== CORE TRAINING PARAMETERS ==============
num_epochs: 200  # Increased for better convergence with new architecture
learning_rate: 0.0001  # Conservative learning rate for stability
gradient_accumulation_steps: 4  # Increased for better gradient stability
batch_size: 24  # Slightly reduced for memory efficiency with larger model
num_workers: 6
prefetch_factor: 4
persistent_workers: true

# ============== DATA CONFIGURATION ==============
data_path: "data/processed/ultimate_dataset.pkl"
train_split: 0.7
val_split: 0.15
test_split: 0.15

# Enhanced validation settings
validation:
  fast_mode: false  # Use full validation for better monitoring
  ddim_steps: 20   # More steps for better sampling quality
  skip_generation: false  # Enable generation for monitoring
  full_validation_every: 5  # Full validation every 5 epochs
  validate_every: 1  # Validate every epoch
  compute_clinical_metrics: true

# ============== IMPROVED MODEL ARCHITECTURE ==============
model:
  base_channels: 64
  n_levels: 4
  n_classes: 5
  dropout: 0.15  # Increased dropout for better regularization
  use_attention: true
  use_film: true
  
  # Enhanced architecture features
  use_multiscale: true
  use_uncertainty: true
  use_robust_loss: true
  temperature_scaling: 1.2  # For better calibration
  
  # Joint generation capabilities
  enable_joint_generation: true
  static_param_ranges:
    age: [-0.36, 11.37]
    intensity: [-2.61, 1.99]
    stimulus_rate: [-6.79, 5.10]
    fmp: [-0.20, 129.11]

# ============== OPTIMIZER SETTINGS ==============
optimizer:
  type: "adamw"
  weight_decay: 5e-4  # Increased regularization
  betas: [0.9, 0.95]  # Adjusted for better convergence
  eps: 1e-8
  
  # Learning rate scheduling per parameter group
  param_groups:
    heads: 
      lr_multiplier: 2.0  # Higher LR for output heads
    encoder:
      lr_multiplier: 0.8  # Lower LR for encoder
    decoder:
      lr_multiplier: 1.0  # Standard LR for decoder

# ============== LEARNING RATE SCHEDULER ==============
scheduler:
  type: "cosine_annealing_warm_restarts"
  T_0: 30  # Longer cycles for stability
  T_mult: 1.5
  eta_min: 5e-7  # Lower minimum
  warmup_epochs: 15  # Longer warmup

# ============== IMPROVED LOSS CONFIGURATION ==============
loss_weights:
  # Rebalanced based on performance analysis
  signal: 0.8  # Slightly reduced as this was performing well
  peak_exist: 1.2  # Increased for better peak detection
  peak_latency: 2.5  # Significantly increased to address NaN issues
  peak_amplitude: 2.5  # Significantly increased to address NaN issues
  classification: 2.0  # Increased for better minority class handling
  threshold: 3.0  # Highest weight to address negative R² issues
  static_params: 1.5  # Weight for joint generation of static parameters

# Enhanced loss configuration
use_class_weights: true
label_smoothing: 0.1
focal_loss:
  enabled: true  # Enable focal loss for classification
  alpha: 1.0
  gamma: 2.0

# Robust loss settings
robust_loss:
  enabled: true
  huber_delta: 1.0  # For peak regression
  threshold_huber_delta: 5.0  # For threshold regression
  
# Peak loss specific settings
peak_loss:
  type: "huber"  # More robust than MSE
  uncertainty_weight: 0.1  # Weight for uncertainty regularization
  existence_pos_weight: 2.0  # Handle class imbalance in peak existence

# Static parameter loss settings (for joint generation)
static_param_loss:
  enabled: true
  type: "huber"  # Robust loss for parameters
  individual_weights:
    age: 1.0
    intensity: 1.2  # Slightly higher as intensity is critical
    stimulus_rate: 1.0
    fmp: 0.8  # Slightly lower as FMP can be more variable

# ============== CURRICULUM LEARNING ==============
curriculum:
  enabled: true
  ramp_epochs: 50  # Longer ramp for stability
  
  # Progressive weight changes
  weights:
    signal: [1.0, 0.8]  # Reduce signal weight over time
    classification: [3.0, 2.0]  # Start high, reduce gradually
    peak_exist: [0.8, 1.2]  # Increase over time
    peak_latency: [1.5, 2.5]  # Significant increase
    peak_amplitude: [1.5, 2.5]  # Significant increase
    threshold: [2.0, 3.0]  # Large increase for problematic threshold prediction
    static_params: [0.5, 1.5]  # Gradually increase static parameter importance
  
  # Task-specific curriculum
  peak_curriculum:
    start_with_easier_samples: true
    existence_first_epochs: 20  # Focus on existence prediction first
    regression_ramp_epochs: 30  # Gradually introduce regression
  
  # Joint generation curriculum
  joint_generation:
    enabled: true
    start_conditional_epochs: 30  # Start with conditional generation only
    introduce_joint_epoch: 50  # Begin joint generation training
    joint_probability: 0.3  # Probability of using joint generation (vs conditional)

# ============== REGULARIZATION ==============
regularization:
  # Gradient clipping
  gradient_clip: 0.5  # More aggressive clipping
  
  # Weight decay scheduling
  weight_decay_schedule:
    enabled: true
    start_weight_decay: 1e-4
    end_weight_decay: 5e-4
    ramp_epochs: 50
  
  # Dropout scheduling
  dropout_schedule:
    enabled: true
    start_dropout: 0.1
    peak_dropout: 0.2
    peak_epoch: 100
    end_dropout: 0.15

# ============== EARLY STOPPING ==============
patience: 40  # Increased patience for complex model
min_delta: 5e-7  # Smaller delta for fine improvements

# ============== MODEL SAVING ==============
save_best_only: false
save_every: 5  # More frequent saves
checkpoint_dir: "checkpoints/production_improved"
save_formats: ["pytorch"]

# Enhanced saving criteria
save_criteria:
  best_loss: true
  best_peak_f1: true  # Save best peak prediction performance
  best_threshold_r2: true  # Save best threshold performance
  best_classification_f1: true  # Save best classification performance
  best_combined_metric: true  # Combined metric considering all tasks
  latest: true
  milestone_epochs: [50, 100, 150, 200]

# ============== ENHANCED VISUALIZATION AND LOGGING ==============
visualization:
  enabled: true
  save_plots: true
  plot_dir: "plots/production_improved"
  plot_every: 2  # More frequent plotting
  
  # Enhanced visualizations
  plot_types:
    - "training_curves"
    - "validation_metrics" 
    - "learning_rate"
    - "loss_components"
    - "loss_weights_evolution"  # Track curriculum learning
    - "sample_reconstructions"
    - "confusion_matrix"
    - "peak_predictions"
    - "threshold_scatter"  # Scatter plot of predicted vs true thresholds
    - "class_performance"  # Per-class performance metrics
    - "uncertainty_calibration"  # If using uncertainty estimation

# Detailed logging configuration
logging:
  level: "INFO" 
  log_dir: "logs/production_improved"
  use_tensorboard: true
  tensorboard_dir: "runs/production_improved"
  save_training_log: true
  
  # Enhanced logging frequency
  log_every: 25  # More frequent logging
  log_metrics_every: 1
  log_histograms_every: 10  # Log weight/gradient histograms
  
  # Specific metrics to track
  track_metrics:
    - "peak_latency_r2"
    - "peak_amplitude_r2" 
    - "threshold_r2"
    - "classification_macro_f1"
    - "peak_existence_f1"
    - "signal_correlation"

# ============== PERFORMANCE OPTIMIZATIONS ==============
use_amp: true  # Mixed precision
use_memory_efficient_attention: true
clear_cache_every: 50  # More frequent cache clearing
pin_memory: true
use_balanced_sampler: true  # Enable balanced sampling for class imbalance

# Gradient accumulation settings
gradient_accumulation:
  adaptive: true  # Adapt based on memory usage
  min_steps: 2
  max_steps: 8

# ============== REPRODUCIBILITY ==============
random_seed: 42
deterministic: false
benchmark: true

# ============== CLINICAL CONSTRAINTS ==============
clinical:
  apply_constraints: true
  peak_latency_range: [1.0, 8.0]  # ms
  amplitude_range: [-0.5, 0.5]   # μV
  threshold_range: [0.0, 120.0]  # dB HL
  
  # Enhanced constraint enforcement
  constraint_loss_weight: 0.1  # Additional loss for constraint violations
  soft_constraints: true  # Use soft constraints during training

# ============== OUTPUT DIRECTORIES ==============
output_dir: "outputs/production_improved"
experiment_name: "abr_improved_architecture_200epochs"

# ============== EVALUATION SETTINGS ==============
evaluation:
  enabled: true
  eval_every: 5  # Comprehensive evaluation every 5 epochs
  save_predictions: true
  compute_clinical_metrics: true
  
  # Enhanced evaluation metrics
  metrics:
    signal_quality: ["correlation", "mse", "spectral_mse"]
    peak_prediction: ["existence_f1", "existence_auc", "latency_r2", "amplitude_r2", "latency_mae", "amplitude_mae"]
    classification: ["accuracy", "balanced_accuracy", "macro_f1", "weighted_f1", "per_class_f1"]
    threshold: ["r2", "mae", "correlation", "percentile_errors"]
    static_params: ["age_r2", "intensity_r2", "stimulus_rate_r2", "fmp_r2", "param_correlation", "param_mse"]  # Joint generation metrics
  
  # Joint generation evaluation
  joint_generation_eval:
    enabled: true
    num_samples: 100  # Number of joint samples to generate for evaluation
    temperature: 1.0  # Temperature for sampling
    use_constraints: true  # Apply clinical constraints
    
    # Evaluation modes
    eval_modes:
      - "conditional"  # Standard conditional generation
      - "joint"  # Joint generation of signals and parameters
      - "unconditional"  # Fully unconditional generation
  
  # Statistical significance testing
  statistical_tests:
    enabled: true
    confidence_level: 0.95
  
# ============== MONITORING AND ALERTS ==============
monitoring:
  enabled: true
  
  # Performance alerts
  alerts:
    peak_latency_nan_check: true
    peak_amplitude_nan_check: true  
    threshold_r2_negative_check: true
    gradient_explosion_check: true
    loss_divergence_check: true
  
  # Early intervention strategies
  interventions:
    reduce_lr_on_nan: true
    restart_on_divergence: true
    save_checkpoint_on_alert: true

# ============== ADVANCED FEATURES ==============
advanced:
  # Model ensemble (if enabled)
  ensemble:
    enabled: false  # Disable for single model training
    num_models: 3
    
  # Knowledge distillation (for future use)
  distillation:
    enabled: false
    
  # Model compression (for deployment)
  compression:
    enabled: false
    quantization: false
    pruning: false 
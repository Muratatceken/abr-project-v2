# Simplified ABR Training Configuration for OptimizedHierarchicalUNet
# Disables problematic features causing gradient flow issues

# ============== EXPERIMENT SETTINGS ==============
experiment:
  name: "abr_simplified_optimized"
  description: "Simplified OptimizedHierarchicalUNet without joint generation to fix gradient flow"
  tags: ["abr", "optimized", "simplified", "gradient-fix"]
  random_seed: 42

# ============== DATA SETTINGS ==============
data:
  data_path: "data/processed/ultimate_dataset.pkl"
  valid_peaks_only: false
  val_split: 0.2
  augment: false  # Disable initially to focus on convergence
  cfg_dropout_prob: 0.1

# ============== MODEL ARCHITECTURE ==============
model:
  # Use optimized architecture but with simplified features
  type: "optimized_hierarchical_unet"
  input_channels: 1
  static_dim: 4
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  signal_length: 200
  num_classes: 5
  
  # Optimized model specific parameters
  num_transformer_layers: 2
  num_heads: 8
  use_multi_scale_attention: true
  use_cross_attention: true
  
  # S4 configuration
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Reduced dropout for better gradient flow
  film_dropout: 0.1
  dropout: 0.05
  use_cfg: true
  
  # Optimized model features
  use_attention_heads: true
  predict_uncertainty: false  # DISABLE to simplify training
  use_task_specific_extractors: true
  use_attention_skip_connections: true
  channel_multiplier: 2.0
  
  # CRITICAL: Disable joint generation to fix gradient flow
  enable_joint_generation: false

# ============== TRAINING SETTINGS ==============
training:
  batch_size: 32
  num_epochs: 200
  learning_rate: 0.0003  # Higher LR for multi-task learning
  weight_decay: 0.0001
  
  use_amp: true
  gradient_clip_norm: 2.0
  
  patience: 25
  save_every: 10
  
  num_workers: 4
  pin_memory: true
  use_balanced_sampler: false

# ============== SIMPLIFIED LOSS FUNCTION ==============
loss:
  # Balanced loss weights for 4 main tasks (no joint generation)
  loss_weights:
    signal: 0.5
    peak_exist: 1.5
    peak_latency: 3.0
    peak_amplitude: 3.0
    classification: 4.0
    threshold: 2.5
  
  # Classification settings
  use_focal_loss: true
  focal_alpha: 1.0
  focal_gamma: 2.0
  use_class_weights: true
  
  # Use robust loss for regressions
  peak_loss_type: "huber"
  huber_delta: 1.0

# ============== OPTIMIZER AND SCHEDULER ==============
optimizer:
  type: "adamw"
  betas: [0.9, 0.95]
  eps: 1e-8

scheduler:
  type: "cosine_warm_restarts"
  T_0: 20
  T_mult: 1.5
  eta_min: 1e-6
  warmup_epochs: 5

# ============== EVALUATION SETTINGS ==============
evaluation:
  metrics:
    - "f1_macro"
    - "f1_weighted"
    - "balanced_accuracy"
    - "confusion_matrix"
  
  validate_every: 1
  save_best_metric: "f1_macro"

# ============== LOGGING ==============
logging:
  output_dir: null
  use_tensorboard: true
  use_wandb: false
  log_level: "INFO"
  log_every: 10

# ============== VALIDATION SETTINGS ==============
validation:
  fast_mode: false
  skip_generation: true  # Skip DDIM generation to focus on direct metrics
  full_validation_every: 1

# ============== CHECKPOINT ==============
checkpoint:
  resume_from: null
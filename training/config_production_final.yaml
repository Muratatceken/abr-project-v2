# Final Production ABR Training Configuration
# Optimized for best performance with comprehensive evaluation and model saving

experiment:
  name: "abr_production_final_v1"
  description: "Final production training with OptimizedHierarchicalUNet using clinical thresholds for maximum performance"
  random_seed: 42
  tags: ["production", "final", "clinical-thresholds", "optimized-architecture", "best-performance"]

# Data Configuration
data:
  data_path: "data/processed/ultimate_dataset_with_clinical_thresholds.pkl"
  valid_peaks_only: false
  val_split: 0.2
  augment: true
  cfg_dropout_prob: 0.1
  normalize_signal: true
  normalize_static: true

# Model Configuration (OptimizedHierarchicalUNet)
model:
  type: "optimized_hierarchical_unet_v2"
  input_channels: 1
  static_dim: 4
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  signal_length: 200
  num_classes: 5
  
  # S4 Configuration (optimized)
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Transformer Configuration (optimized for production)
  num_transformer_layers: 3  # Increased for better performance
  num_heads: 8
  use_multi_scale_attention: true
  use_cross_attention: true
  
  # FiLM and Conditioning
  dropout: 0.1
  film_dropout: 0.1  # Reduced for production
  use_cfg: true
  
  # Output Configuration
  use_attention_heads: true
  predict_uncertainty: true
  
  # Joint Generation
  enable_joint_generation: true
  static_param_ranges:
    age: [-2.0, 2.0]
    intensity: [-2.0, 2.0]
    stimulus_rate: [-2.0, 2.0]
    fmp: [0.0, 150.0]
  
  # Optimization Features
  use_task_specific_extractors: true
  use_attention_skip_connections: true
  channel_multiplier: 2.0

# Training Configuration (Production Optimized)
training:
  batch_size: 16  # Increased for better gradient estimates
  num_epochs: 150  # Extended for thorough training
  learning_rate: 0.0002  # Conservative for stable training
  weight_decay: 1e-4
  use_amp: true  # Enable for efficiency (our FFT fix handles this)
  patience: 25  # Generous patience for production
  gradient_clip_norm: 1.0
  gradient_accumulation_steps: 2  # Effective batch size of 32
  num_workers: 4
  
  # Advanced training settings
  warmup_epochs: 10
  save_every: 5  # Save more frequently
  validate_every: 1
  
  # Task-specific learning rate multipliers
  task_lr_multipliers:
    task_extractors: 1.0
    attention_layers: 0.9
    s4_layers: 1.1
    other: 1.0

# Loss Configuration (Production Tuned)
loss:
  use_focal_loss: true
  use_class_weights: true
  peak_loss_type: "mse"
  use_log_threshold: false
  use_uncertainty_threshold: true
  enable_static_param_loss: true
  
  # Optimized loss weights for production
  loss_weights:
    signal: 1.0
    peak_exist: 0.8
    peak_latency: 2.5  # High emphasis on timing
    peak_amplitude: 2.0
    classification: 3.0  # High emphasis on classification
    threshold: 4.0  # Maximum emphasis on clinical thresholds
    joint_generation: 1.0

# Optimizer Configuration (Production)
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8
  amsgrad: false

# Learning Rate Scheduler (Multi-stage)
scheduler:
  type: "cosine_with_restarts"
  warmup_steps: 2000  # Extended warmup
  T_0: 30  # First restart after 30 epochs
  T_mult: 2  # Double restart period each time
  eta_min: 1e-7  # Very low minimum
  eta_max_factor: 1.0

# CFG Configuration
cfg:
  joint_cfg:
    enabled: true
    guidance_scale: 7.5
    dropout_prob: 0.1

# Hardware Configuration (Production)
hardware:
  compile_model: true
  use_flash_attention: true
  memory_fraction: 0.95
  benchmark: true
  deterministic: false

# Validation Configuration (Balanced)
validation:
  fast_mode: false  # Full validation for production
  full_validation_every: 5  # Full evaluation every 5 epochs
  ddim_steps: 30  # Balanced quality vs speed
  skip_generation: false  # Include generation evaluation
  use_cv: false
  cv_folds: 5
  cv_strategy: "StratifiedGroupKFold"
  
  # Advanced validation settings
  compute_clinical_metrics: true
  save_predictions: true
  save_attention_maps: true

# Logging Configuration (Comprehensive)
logging:
  output_dir: "runs/production_final"
  use_wandb: false  # Set to true if you have wandb
  wandb_project: "abr-production-final"
  use_tensorboard: true
  log_every: 20
  save_every: 5
  save_formats: ["pytorch", "onnx"]  # Export to ONNX for deployment
  
  # Enhanced logging
  log_gradients: true
  log_weights: true
  log_learning_rate: true

# Early Stopping Configuration (Production)
early_stopping:
  enabled: true
  monitor: "val_loss"  # Primary metric
  mode: "min"
  patience: 25  # Patient for production
  min_delta: 1e-5  # Sensitive to small improvements
  restore_best_weights: true
  
  # Secondary metrics monitoring
  secondary_monitors:
    - metric: "val_classification_f1"
      mode: "max"
      weight: 0.3
    - metric: "val_threshold_mae"
      mode: "min"
      weight: 0.2

# Model Checkpointing (Comprehensive)
checkpointing:
  save_best: true
  save_last: true
  save_top_k: 3  # Keep top 3 models
  monitor: "val_loss"
  mode: "min"
  
  # Multiple checkpoint strategies
  checkpoint_strategies:
    - name: "best_loss"
      monitor: "val_loss"
      mode: "min"
      save_path: "checkpoints/production_final/best_loss.pth"
    - name: "best_f1"
      monitor: "val_classification_f1"
      mode: "max"
      save_path: "checkpoints/production_final/best_f1.pth"
    - name: "best_threshold"
      monitor: "val_threshold_mae"
      mode: "min"
      save_path: "checkpoints/production_final/best_threshold.pth"
    - name: "latest"
      save_every: 10
      save_path: "checkpoints/production_final/latest_epoch_{epoch}.pth"

# Evaluation Configuration (Production)
evaluation:
  architecture_evaluation: true
  compute_metrics: true
  save_predictions: true
  create_visualizations: true
  
  # Enhanced evaluation
  compute_confidence_intervals: true
  bootstrap_samples: 1000
  save_misclassified_samples: true
  generate_classification_report: true

# Output Configuration (Production)
output:
  save_attention_maps: true
  save_feature_maps: true
  save_generated_samples: true
  num_sample_generations: 50  # More samples for production
  
  # Advanced outputs
  save_model_summary: true
  save_training_curves: true
  save_confusion_matrices: true
  export_model_graph: true

# Curriculum Learning (Production)
curriculum:
  enabled: true
  ramp_epochs: 15  # Longer ramp for production
  strategy: "exponential"
  
  # Task-specific curriculum
  task_schedules:
    signal: 
      start_epoch: 0
      ramp_epochs: 5
      start_weight: 0.5
      end_weight: 1.0
    classification:
      start_epoch: 3
      ramp_epochs: 10
      start_weight: 1.5
      end_weight: 3.0
    threshold:
      start_epoch: 8
      ramp_epochs: 15
      start_weight: 2.0
      end_weight: 4.0
    peak_latency:
      start_epoch: 5
      ramp_epochs: 12
      start_weight: 1.0
      end_weight: 2.5

# Advanced Settings (Production)
advanced:
  memory_efficient_attention: true
  gradient_checkpointing: false  # Disabled for speed in production
  mixed_precision_level: "O1"
  distributed_training: false
  
  # Model-specific optimizations
  use_fused_layernorm: true
  use_fused_adam: true
  use_channels_last: true  # Memory layout optimization
  
  # Debugging and monitoring
  detect_anomaly: false
  profile_memory: false
  monitor_gradients: true
  gradient_norm_tracking: true

# Data Augmentation (Production)
augmentation:
  enabled: true
  probability: 0.3
  
  # Signal augmentations
  noise_std: 0.05
  amplitude_scaling: [0.8, 1.2]
  time_stretching: [0.95, 1.05]
  frequency_masking: 0.1
  
  # Static parameter augmentations
  static_noise_std: 0.02

# Clinical Validation (Production)
clinical_validation:
  enabled: true
  thresholds:
    hearing_loss_mild: 25
    hearing_loss_moderate: 40
    hearing_loss_severe: 70
    hearing_loss_profound: 90
  
  # Clinical metrics
  compute_sensitivity: true
  compute_specificity: true
  compute_ppv: true
  compute_npv: true
  confidence_level: 0.95
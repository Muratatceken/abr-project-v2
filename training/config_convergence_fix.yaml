# ABR Training Configuration - Convergence Fix
# Addresses critical loss convergence issues

# ============== EXPERIMENT SETTINGS ==============
experiment:
  name: "abr_convergence_fix"
  description: "Fixed loss weights and training settings for proper convergence"
  tags: ["abr", "convergence-fix", "balanced-losses"]
  random_seed: 42

# ============== DATA SETTINGS ==============
data:
  data_path: "data/processed/ultimate_dataset.pkl"
  valid_peaks_only: false
  val_split: 0.2
  augment: false  # Disable initially to focus on convergence
  cfg_dropout_prob: 0.1

# ============== MODEL ARCHITECTURE ==============
model:
  # Use optimized architecture for better convergence
  type: "optimized_hierarchical_unet"
  input_channels: 1
  static_dim: 4
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  signal_length: 200
  num_classes: 5
  
  # Optimized model specific parameters
  num_transformer_layers: 2  # Optimized uses fewer transformer layers
  num_heads: 8
  use_multi_scale_attention: true  # Optimized model feature
  use_cross_attention: true
  
  # S4 configuration
  s4_state_size: 64
  num_s4_layers: 2
  use_enhanced_s4: true
  
  # Reduced dropout for better gradient flow
  film_dropout: 0.1
  dropout: 0.05
  use_cfg: true
  
  # Optimized model features
  use_attention_heads: true
  predict_uncertainty: true  # Optimized model handles this better
  use_task_specific_extractors: true  # Key optimization feature
  use_attention_skip_connections: true  # Key optimization feature
  channel_multiplier: 2.0
  
  # Joint generation parameters
  enable_joint_generation: true

# ============== TRAINING SETTINGS (CONVERGENCE FOCUSED) ==============
training:
  # Larger batch size for more stable gradients
  batch_size: 32
  num_epochs: 200
  
  # CRITICAL: Higher learning rate for multi-task learning
  learning_rate: 3e-4  # Increased from 1e-4
  weight_decay: 1e-4  # Reduced weight decay
  
  # Mixed precision and optimization
  use_amp: true
  gradient_clip_norm: 2.0  # Increased gradient clipping
  
  # More patient early stopping
  patience: 25
  save_every: 10
  
  # Data loading
  num_workers: 4
  pin_memory: true
  
  # Disable balanced sampler initially
  use_balanced_sampler: false

# ============== FIXED LOSS FUNCTION SETTINGS ==============
loss:
  # CRITICAL: Rebalanced loss weights based on typical loss magnitudes
  loss_weights:
    signal: 0.5          # Reduced - signal reconstruction converges easily
    peak_exist: 1.5      # Increased - binary classification needs attention
    peak_latency: 3.0    # SIGNIFICANTLY increased - regression task
    peak_amplitude: 3.0  # SIGNIFICANTLY increased - regression task
    classification: 4.0  # HIGHEST weight - main task with class imbalance
    threshold: 2.5       # High weight - another regression task
  
  # DISABLE adaptive loss weighting that causes instability
  use_adaptive_weighting: false
  
  # Classification loss settings
  use_focal_loss: true   # ENABLE focal loss for class imbalance
  focal_alpha: 1.0
  focal_gamma: 2.0
  use_class_weights: true
  
  # Use more robust loss for regressions
  peak_loss_type: "huber"  # More stable than MSE
  huber_delta: 1.0

# ============== IMPROVED OPTIMIZER AND SCHEDULER ==============
optimizer:
  type: "adamw"
  betas: [0.9, 0.95]  # Slightly more aggressive momentum
  eps: 1e-8

scheduler:
  type: "cosine_warm_restarts"
  # Longer warm restart cycles for stability
  T_0: 20  # Increased from 10
  T_mult: 1.5  # Reduced from 2 for more frequent restarts
  eta_min: 1e-6
  # Longer warmup for stable start
  warmup_epochs: 5

# ============== EVALUATION SETTINGS ==============
evaluation:
  metrics:
    - "f1_macro"
    - "f1_weighted"
    - "balanced_accuracy"
    - "confusion_matrix"
  
  validate_every: 1
  save_best_metric: "f1_macro"

# ============== LOGGING AND MONITORING ==============
logging:
  output_dir: null
  use_tensorboard: true
  use_wandb: false
  log_level: "INFO"
  log_every: 10

# ============== VALIDATION SETTINGS ==============
validation:
  fast_mode: false  # Use full validation to monitor all metrics
  skip_generation: true  # Skip DDIM generation to focus on direct metrics
  full_validation_every: 1

# ============== CHECKPOINT AND RESUMING ==============
checkpoint:
  resume_from: null
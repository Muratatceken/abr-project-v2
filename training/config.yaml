# Enhanced ABR Training Configuration
# Comprehensive settings for ProfessionalHierarchicalUNet training

# ============== EXPERIMENT SETTINGS ==============
experiment:
  name: "enhanced_abr_v1"
  description: "Enhanced ABR model with CFG, cross-attention, and multi-task learning"
  tags: ["abr", "multi-task", "cfg", "cross-attention"]
  random_seed: 42

# ============== DATA SETTINGS ==============
data:
  data_path: "data/processed/ultimate_dataset.pkl"
  valid_peaks_only: false  # Set to true to train only on samples with valid V peaks
  val_split: 0.2
  augment: true
  cfg_dropout_prob: 0.1  # Probability of unconditional training for CFG

# ============== MODEL ARCHITECTURE ==============
model:
  # Basic architecture
  input_channels: 1
  static_dim: 4
  base_channels: 64
  n_levels: 4
  sequence_length: 200
  signal_length: 200
  num_classes: 5
  
  # Enhanced features
  num_transformer_layers: 3
  num_heads: 8
  use_cross_attention: true
  use_positional_encoding: true
  positional_type: "sinusoidal"  # or "learned"
  
  # Conditioning and regularization
  film_dropout: 0.15
  dropout: 0.1
  use_cfg: true
  use_multi_film: true
  
  # Advanced features
  use_attention_heads: true
  predict_uncertainty: false
  channel_multiplier: 2.0
  use_enhanced_s4: true
  use_learnable_timescales: true

# ============== TRAINING SETTINGS ==============
training:
  # Basic training parameters
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.0001  # Changed from 1e-4 to avoid string parsing issues
  weight_decay: 0.01
  
  # Mixed precision and optimization
  use_amp: true
  gradient_clip_norm: 1.0
  
  # Early stopping and patience
  patience: 15
  save_every: 10
  
  # Data loading
  num_workers: 4
  pin_memory: true
  
  # Sampling strategy
  use_balanced_sampler: false  # Set to true for severely imbalanced classes

# ============== LOSS FUNCTION SETTINGS ==============
loss:
  # Loss component weights
  loss_weights:
    signal: 1.0
    peak_exist: 0.5
    peak_latency: 1.0
    peak_amplitude: 1.0
    classification: 1.5  # Higher weight for main task
    threshold: 0.8
  
  # Classification loss settings
  use_focal_loss: false  # Set to true for severe class imbalance
  focal_alpha: 1.0
  focal_gamma: 2.0
  use_class_weights: true
  
  # Peak loss settings
  peak_loss_type: "mse"  # "mse", "mae", "huber", "smooth_l1"
  huber_delta: 1.0

# ============== OPTIMIZER AND SCHEDULER ==============
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  type: "cosine_warm_restarts"  # "cosine_warm_restarts", "reduce_on_plateau", "step", null
  # Cosine warm restarts parameters
  T_0: 10
  T_mult: 2
  eta_min: 1e-6
  # Reduce on plateau parameters
  factor: 0.5
  patience: 5
  # Step scheduler parameters
  step_size: 20
  gamma: 0.5

# ============== EVALUATION SETTINGS ==============
evaluation:
  metrics:
    - "f1_macro"
    - "f1_weighted"
    - "balanced_accuracy"
    - "confusion_matrix"
  
  # Validation settings
  validate_every: 1
  save_best_metric: "f1_macro"  # or "total_loss"

# ============== LOGGING AND MONITORING ==============
logging:
  # Output directory
  output_dir: null  # Will auto-generate if null
  
  # TensorBoard
  use_tensorboard: true
  
  # Weights & Biases
  use_wandb: false
  wandb_project: "abr-enhanced-training"
  wandb_entity: null
  
  # Console logging
  log_level: "INFO"
  log_every: 10  # Log every N batches during training

# ============== INFERENCE AND CFG SETTINGS ==============
inference:
  # CFG settings for enhanced generation
  cfg_guidance_scale: 1.5
  task_specific_scales:
    recon: 1.0
    peak: 0.8
    class: 1.2
    threshold: 0.9
  
  # Sampling settings
  temperature: 1.0
  apply_clinical_constraints: true
  
  # Batch inference settings
  inference_batch_size: 64

# ============== CHECKPOINT AND RESUMING ==============
checkpoint:
  resume_from: null  # Path to checkpoint to resume from
  save_best_only: false
  save_last: true
  save_optimizer_state: true
  save_scheduler_state: true

# ============== HARDWARE AND PERFORMANCE ==============
hardware:
  # GPU settings
  gpu_ids: [0]  # List of GPU IDs to use
  distributed: false
  
  # Memory and performance
  dataloader_pin_memory: true
  benchmark: true  # Set torch.backends.cudnn.benchmark = True
  deterministic: false  # Set torch.backends.cudnn.deterministic = True for reproducibility

# ============== ADVANCED TRAINING TECHNIQUES ==============
advanced:
  # Gradient accumulation
  gradient_accumulation_steps: 1

  # Label smoothing
  label_smoothing: 0.0

  # Mixup and CutMix (experimental)
  use_mixup: false
  mixup_alpha: 0.2

  # Test time augmentation
  use_tta: false
  tta_steps: 5

  # Curriculum learning for multitask stability
  curriculum:
    enabled: true
    peak_start: 5        # Start peak prediction loss at epoch 5
    threshold_start: 10  # Start threshold loss at epoch 10
    class_start: 3       # Start classification loss at epoch 3
    ramp_epochs: 5       # Number of epochs to ramp up to full weight

# ============== CLASS-SPECIFIC SETTINGS ==============
classes:
  # Class names (must match label encoder)
  names: ["NORMAL", "NÖROPATİ", "SNİK", "TOTAL", "İTİK"]
  
  # Class-specific settings
  class_weights: null  # Will be computed automatically if null
  
  # Minority class handling
  oversample_minority: false
  undersample_majority: false
  
  # Threshold adjustment for imbalanced classes
  adjust_thresholds: false

# ============== VALIDATION AND TESTING ==============
validation:
  # Cross-validation settings
  use_cv: false
  cv_folds: 5
  cv_strategy: "StratifiedGroupKFold"  # "StratifiedKFold", "StratifiedGroupKFold", "GroupKFold"
  cv_group_column: "patient_id"  # Column to use for grouping (prevents data leakage)

  # Validation augmentation
  val_augment: false

  # Test set evaluation
  test_split: 0.0  # Additional test split if > 0

  # Cross-validation specific settings
  cv_save_all_folds: false  # Save models from all folds
  cv_ensemble_prediction: true  # Use ensemble prediction from all folds

# ============== CLINICAL CONSTRAINTS ==============
clinical:
  # Peak constraints (physiologically plausible ranges)
  peak_latency_range: [1.0, 8.0]  # ms
  peak_amplitude_range: [-2.0, 2.0]  # μV (normalized)
  
  # Threshold constraints
  threshold_range: [0.0, 120.0]  # dB
  
  # Signal constraints
  signal_amplitude_range: [-1.0, 1.0]  # Normalized range
  
  # Apply constraints during training
  apply_during_training: false
  apply_during_inference: true 